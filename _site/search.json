[
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Cleansing_Projects/project3.html",
    "href": "Cleansing_Projects/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Projects/project1.html",
    "href": "Cleansing_Projects/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project4.html",
    "href": "Cleansing_Projects/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Full_Stack/project2.html",
    "href": "Full_Stack/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Full_Stack/project5.html",
    "href": "Full_Stack/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Competition/project3.html",
    "href": "Competition/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Competition/project1.html",
    "href": "Competition/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Competition/project4.html",
    "href": "Competition/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project2.html",
    "href": "Cleansing_Exploration/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project5.html",
    "href": "Cleansing_Exploration/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Machine_Learning/project2.html",
    "href": "Machine_Learning/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine_Learning/project5.html",
    "href": "Machine_Learning/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Story_Telling/project2.html",
    "href": "Story_Telling/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Story_Telling/project5.html",
    "href": "Story_Telling/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Story_Telling/project4.html",
    "href": "Story_Telling/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Story_Telling/project1.html",
    "href": "Story_Telling/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Story_Telling/project3.html",
    "href": "Story_Telling/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Tucker Trost",
    "section": "",
    "text": "üìû (971) 420-5397 | üìß tuckertrost16@gmail.com\nüîó LinkedIn | üåê tuckertrostbyui.github.io\n\n\n\nBachelor of Science, Data Science\nBrigham Young University - Idaho ‚Äî Rexburg, Idaho\nFebruary 2025 ‚Äì April 2025\nRelevant Courses: Machine Learning, Data Visualization, Data Wrangling, Statistics, SQL, Python, R, Pyspark\nSocieties/Clubs: President of the Data Science Society\n\n\n\n\nData Analyst, Student Records and Registration\nBrigham Young University-Idaho ‚Äî Idaho\nOctober 2024 ‚Äì Present\n- Built and maintained a records retention Power BI Dashboard identifying over 400,000 student records ready for disposal\n- Utilized SQL to pull data from multiple sources to pipeline into dashboard for automation\n- Presented dashboard updates to executives biweekly for feedback and stakeholder alignment\nData Science Consultant, BYU-I Career Center\nBrigham Young University-Idaho ‚Äî Idaho\nApril 2025 ‚Äì Present\n- Built an end-to-end data pipeline and Power BI dashboard to track alumni career outcomes\n- Wrote SQL and Python code to pull, clean, and process data from multiple sources, including NLP on job titles\n- Met regularly with stakeholders to align on goals, share updates, and deliver actionable insights\nProject Manager, Data Science Society\nBrigham Young University-Idaho ‚Äî Idaho\nJanuary 2025 ‚Äì April 2025\n- Led a team of data scientists using a SQL Database to house data for an attendance analytics project for the Pioneer Baseball League\n- Created compelling Data Visualizations in Python for stakeholders\n- Met once a month with league commissioner to communicate insights and receive feedback\nData Science Tutor/Teaching Assistant\nBrigham Young University-Idaho ‚Äî Idaho\nJanuary 2025 ‚Äì April 2025\n- Guided students of all skill levels in Python, SQL, R, Tableau, and Power BI\n- Broke down data wrangling and visualization concepts to enhance comprehension beyond assignments\n- Adapted explanations to diverse learning styles, fostering problem-solving skills and confidence in data analysis\n\n\n\n\nAnalysis/Visualization: Python (Pandas, Lets-Plot, plotly, Pyspark), R (ggplot, tidyverse), Power BI (DAX), Tableau\nMachine Learning: Scikit-learn, TensorFlow, Random Forest Classifiers, Neural Networks, XGBoost\nDatabase Management/Design: MySQL, SQLite, Databricks, DAX Studio, Microsoft SQL Server\n\n\n\n\nStudent Records Retention Compliance Dashboard\nOctober 2024 ‚Äì Present\n- Developed a Power BI model that identified over 200,000 student records ready for disposal to comply with AACRAO retention guidelines\n- Wrangled and cleaned data from 5 different databases to implement into the Power BI Model\nPortland Crime Forecasting with XGBoost\nApril 2025\n- Trained an XGBoost model to predict daily Portland, Oregon crime counts with an R¬≤ score of 0.62\n- Applied feature engineering, hyperparameter tuning, and model evaluation with Python to improve prediction accuracy and reveal crime patterns"
  },
  {
    "objectID": "resume.html#currently",
    "href": "resume.html#currently",
    "title": "Tucker Trost‚Äôs CV",
    "section": "",
    "text": "Studying to earn my Bachleor‚Äôs degree in Data Science from Brigham Young University - Idaho\n\n\nPython | SQL | Tableau | Microsoft Excel\n\n\n\nExploratory Data Analysis, Data Visualizations, Interactive Dashboards, Machine Learning"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Tucker Trost",
    "section": "",
    "text": "Bachelor of Science, Data Science\nBrigham Young University - Idaho ‚Äî Rexburg, Idaho\nFebruary 2025 ‚Äì April 2025\nRelevant Courses: Machine Learning, Data Visualization, Data Wrangling, Statistics, SQL, Python, R, Pyspark\nSocieties/Clubs: President of the Data Science Society"
  },
  {
    "objectID": "resume.html#awards",
    "href": "resume.html#awards",
    "title": "Tucker Trost‚Äôs CV",
    "section": "",
    "text": "2023 Google Data Analystics Professional Certificate\nCoursera"
  },
  {
    "objectID": "resume.html#publications",
    "href": "resume.html#publications",
    "title": "Isaac Newtons‚Äôs CV",
    "section": "",
    "text": "1669 Newton Sir I, De analysi per √¶quationes numero terminorum infinitas.\n1669 Lectiones optic√¶.\netc. etc. etc.\n\n\n\n2012 Infinitesimal calculus for solutions to physics problems, SMBC patent 001"
  },
  {
    "objectID": "resume.html#occupation",
    "href": "resume.html#occupation",
    "title": "Isaac Newtons‚Äôs CV",
    "section": "",
    "text": "1600 Royal Mint, London\n\nWarden\nMinted coins\n\n1600 Lucasian professor of Mathematics, Cambridge University"
  },
  {
    "objectID": "Machine_Learning/project4.html",
    "href": "Machine_Learning/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine_Learning/project1.html",
    "href": "Machine_Learning/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine_Learning/project3.html",
    "href": "Machine_Learning/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Cleansing_Exploration/project4.html",
    "href": "Cleansing_Exploration/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project1.html",
    "href": "Cleansing_Exploration/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project3.html",
    "href": "Cleansing_Exploration/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Competition/project5.html",
    "href": "Competition/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Competition/project2.html",
    "href": "Competition/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Full_Stack/project4.html",
    "href": "Full_Stack/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Full_Stack/project1.html",
    "href": "Full_Stack/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Full_Stack/project3.html",
    "href": "Full_Stack/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Projects/project5.html",
    "href": "Cleansing_Projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Projects/project2.html",
    "href": "Cleansing_Projects/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Cleansing_Projects/project1.html#data-source",
    "href": "Cleansing_Projects/project1.html#data-source",
    "title": "Portland Crime Data Cleansing",
    "section": "Data Source",
    "text": "Data Source\nThe dateset that I am using was accessed directly from Portland Police Bureau‚Äôs Open Data initiative; compiled from 2015-2023. This dataset is being used under this license.",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project1.html#data-dictionary",
    "href": "Cleansing_Projects/project1.html#data-dictionary",
    "title": "Portland Crime Data Cleansing",
    "section": "Data Dictionary",
    "text": "Data Dictionary\nAddress: Address of reported incident at the 100 block level (e.g.: 1111 SW 2nd Ave would be 1100 Block SW 2nd Ave).\nCase Number: The case year and number for the reported incident (YY-######).\nCrime Against: Crime against category (Person, Property, or Society).\nNeighborhood: Neighborhood where incident occurred. If the neighborhood name is missing, the incident occurred outside of the boundaries of the Portland neighborhoods or at a location that could not be assigned to a specific address in the system. (e.g., Portland, near Washington Park, on the streetcar, etc.).\nOccur Date: Date the incident occurred. The exact occur date is sometimes unknown. In most situations, the first possible date the crime could have occurred is used as the occur date. (For example, victims return home from a week-long vacation to find their home burglarized. The burglary could have occurred at any point during the week. The first date of their vacation would be listed as the occur date.)\nOccur Time: Time the incident occurred. The exact occur time is sometimes unknown. In most situations, the first possible time the crime could have occurred is used as the occur time. The time is reported in the 24-hour clock format, with the first two digits representing hour (ranges from 00 to 23) and the second two digits representing minutes (ranges from 00 to 59).\nOffense Category: Category of offense (for example, Assault Offenses).\nOffense Type: Type of offense (for example, Aggravated Assault)Note: The statistic for Homicide Offenses has been updated in the Group A Crimes report to align with the 2019 FBI NIBRS definitions. The statistic for Homicide Offenses includes (09A) Murder & Non-negligent Manslaughter and (09B) Negligent Manslaughter. As of January 1, 2019, the FBI expanded the definition of negligent manslaughter to include traffic fatalities that result in an arrest for driving under the influence, distracted driving, or reckless driving. The change in definition impacts the 2019 homicide offenses statistic and the comparability of 2019 homicide statistics to prior year.\nOpen Data Lat/Lon: Generalized Latitude / Longitude of the reported incident. For offenses that occurred at a specific address, the point is mapped to the block‚Äôs midpoint. Offenses that occurred at an intersection is mapped to the intersection centroid.\nOpen Data X/Y: Generalized XY point of the reported incident. For offenses that occurred at a specific address, the point is mapped to the block‚Äôs midpoint. Offenses that occurred at an intersection is mapped to the intersection centroid. To protect the identity of victims and other privacy concerns, the points of certain case types are not released. XY points use the Oregon State Plane North (3601), NAD83 HARN, US International Feet coordinate system.\nOffense Count: Number of offenses per incident. Offenses (i.e.¬†this field) are summed for counting purposes.\n\nPrepare\n\n\nRead and format project data\n# Load in Data\npcrime_15 = pd.read_csv('CrimeData-2015.csv')\npcrime_16 = pd.read_csv('CrimeData-2016.csv')\npcrime_17 = pd.read_csv('CrimeData-2017.csv')\npcrime_18 = pd.read_csv('CrimeData-2018.csv')\npcrime_19 = pd.read_csv('CrimeData-2019.csv')\npcrime_20 = pd.read_csv('CrimeData-2020.csv')\npcrime_21 = pd.read_csv('CrimeData-2021.csv')\npcrime_22 = pd.read_csv('CrimeData-2022.csv')\npcrime_23 = pd.read_csv('CrimeData-2023.csv')\n\n# Combine Datasets\npcrime_combined = pd.concat([pcrime_15,pcrime_16,pcrime_17,pcrime_18,pcrime_19,pcrime_20,pcrime_21,pcrime_22,pcrime_23], ignore_index=True)\npcrime_combined.head()\n\n\n\n\n\n\n\n\n\n\nAddress\nCaseNumber\nCrimeAgainst\nNeighborhood\nOccurDate\nOccurTime\nOffenseCategory\nOffenseType\nOpenDataLat\nOpenDataLon\nOpenDataX\nOpenDataY\nReportDate\nOffenseCount\n\n\n\n\n0\nNaN\n15-X197430\nPerson\nPiedmont\n5/12/2015\n1400\nAssault Offenses\nIntimidation\nNaN\nNaN\nNaN\nNaN\n5/12/2015\n1\n\n\n1\nNaN\n15-X4282999\nPerson\nBuckman West\n5/1/2015\n2143\nAssault Offenses\nSimple Assault\nNaN\nNaN\nNaN\nNaN\n5/1/2015\n1\n\n\n2\nNaN\n15-X4283033\nPerson\nUniversity Park\n5/1/2015\n1625\nAssault Offenses\nSimple Assault\nNaN\nNaN\nNaN\nNaN\n5/1/2015\n1\n\n\n3\nNaN\n15-X4283218\nPerson\nMadison South\n5/1/2015\n1820\nAssault Offenses\nSimple Assault\nNaN\nNaN\nNaN\nNaN\n5/1/2015\n1\n\n\n4\nNaN\n15-X4283218\nPerson\nMadison South\n5/1/2015\n1820\nKidnapping/Abduction\nKidnapping/Abduction\nNaN\nNaN\nNaN\nNaN\n5/1/2015\n1",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project1.html#what-is-our-goal",
    "href": "Cleansing_Projects/project1.html#what-is-our-goal",
    "title": "Portland Crime Data Cleansing",
    "section": "What is our goal?",
    "text": "What is our goal?\nThinking forward to the analysis that I want to perform with this data, I need to understand what I am looking for when it comes to cleaning. I know that I want to focus my analysis on the distribution of different offense types and categories across the various neighborhoods of Portland. Additionally, I‚Äôd like to get insight into the temporal trends that lie within the data. Based on this understanding, I get a better sense of what aspects of the data need to be cleaned.\n\n\nIdentify Missing Data\npcrime_combined.isna().sum()\n\n\nAddress            44998\nCaseNumber             0\nCrimeAgainst           0\nNeighborhood       17566\nOccurDate              0\nOccurTime              0\nOffenseCategory        0\nOffenseType            0\nOpenDataLat        56511\nOpenDataLon        56511\nOpenDataX          56511\nOpenDataY          56511\nReportDate             0\nOffenseCount           0\ndtype: int64",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project1.html#initial-observations",
    "href": "Cleansing_Projects/project1.html#initial-observations",
    "title": "Portland Crime Data Cleansing",
    "section": "Initial Observations",
    "text": "Initial Observations\n1. A time to report column would be useful * Convert OccurDate and ReportDate to datetime * Create a time to report column\n2. OpenDataX/Y don‚Äôt seem necessary for our analysis * Drop OpenDataX and OpenDataY columns\n3. Address column seems to be redundant as most entries are just a general location * Drop Address column\n4. Neighborhood averages can be used to find lat/lon * Drop rows with missing Neighborhood and OpenDataLat * Replace all rows with neighborhood but missing Lat/Lon data with average Lat/Lon of their neighborhood\n\nData Cleaning\n\n\nCleaning\n# Calculate average Lat/Lon for each neighborhood\nneighborhood_means = pcrime_combined.groupby('Neighborhood')[['OpenDataLat','OpenDataLon']].transform('mean')\n\n# Clean the data\npcrime_cleaned = (\n  pcrime_combined\n  .drop(columns=['Address','OpenDataX','OpenDataY']) # Drop X/Y\n  .dropna(subset=['OpenDataLat','Neighborhood'], how='all') # Drop missing lat/lon and Neighborhoods\n  .assign(\n    OccurDate = pd.to_datetime(pcrime_combined['OccurDate']), # Convert dates to datetime\n    ReportDate = pd.to_datetime(pcrime_combined['ReportDate']),\n    ReportDiff = lambda x: (x['ReportDate'] - x['OccurDate']).dt.days, # Calculate time to report\n    OpenDataLat = lambda x: x['OpenDataLat'].fillna(neighborhood_means['OpenDataLat']), # Fill missing Lat/Lon with average Lat/Lon of given neighborhood\n    OpenDataLon = lambda x: x['OpenDataLon'].fillna(neighborhood_means['OpenDataLon'])\n)\n)\n\npcrime_cleaned\n\n\n\n\n\n\n\n\n\n\nCaseNumber\nCrimeAgainst\nNeighborhood\nOccurDate\nOccurTime\nOffenseCategory\nOffenseType\nOpenDataLat\nOpenDataLon\nReportDate\nOffenseCount\nReportDiff\n\n\n\n\n0\n15-X197430\nPerson\nPiedmont\n2015-05-12\n1400\nAssault Offenses\nIntimidation\n45.575321\n-122.669950\n2015-05-12\n1\n0\n\n\n1\n15-X4282999\nPerson\nBuckman West\n2015-05-01\n2143\nAssault Offenses\nSimple Assault\n45.517973\n-122.659334\n2015-05-01\n1\n0\n\n\n2\n15-X4283033\nPerson\nUniversity Park\n2015-05-01\n1625\nAssault Offenses\nSimple Assault\n45.580393\n-122.727295\n2015-05-01\n1\n0\n\n\n3\n15-X4283218\nPerson\nMadison South\n2015-05-01\n1820\nAssault Offenses\nSimple Assault\n45.540839\n-122.578812\n2015-05-01\n1\n0\n\n\n4\n15-X4283218\nPerson\nMadison South\n2015-05-01\n1820\nKidnapping/Abduction\nKidnapping/Abduction\n45.540839\n-122.578812\n2015-05-01\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n531881\n23-225440\nSociety\nSylvan-Highlands\n2023-08-27\n1420\nWeapon Law Violations\nWeapons Law Violations\n45.508945\n-122.731195\n2023-08-27\n1\n0\n\n\n531882\n23-51637\nProperty\nArlington Heights\n2023-02-23\n330\nMotor Vehicle Theft\nMotor Vehicle Theft\n45.506744\n-122.713355\n2023-02-25\n1\n2\n\n\n531883\n23-227984\nPerson\nGoose Hollow\n2023-08-30\n920\nAssault Offenses\nAggravated Assault\n45.515555\n-122.693709\n2023-08-30\n1\n0\n\n\n531884\n23-40689\nProperty\nForest Park\n2023-02-13\n1130\nMotor Vehicle Theft\nMotor Vehicle Theft\n45.540437\n-122.736728\n2023-02-13\n1\n0\n\n\n531885\n23-137815\nProperty\nForest Park\n2023-05-23\n1300\nLarceny Offenses\nTheft From Motor Vehicle\n45.540437\n-122.736728\n2023-05-26\n1\n3\n\n\n\n\n522201 rows √ó 12 columns\n\n\n\n\nNow we can check and see how we did filling in our missing data.\n\n\nCheck missing data\npcrime_cleaned.isna().sum()\n\n\nCaseNumber            0\nCrimeAgainst          0\nNeighborhood       7881\nOccurDate             0\nOccurTime             0\nOffenseCategory       0\nOffenseType           0\nOpenDataLat           0\nOpenDataLon           0\nReportDate            0\nOffenseCount          0\nReportDiff            0\ndtype: int64\n\n\n\n\nFinal Thoughts\nWe have now cleaned our data into a useable state for our analysis. We went from many missing rows from in many columns to only 7881 missing rows in the neighborhood column. Since we have all of the Latitude and Longitude data for each of these missing rows, the missing data will still be useable for our visualizations in Tableau.\nNote: Further cleaning of the missing neighborhood rows could be done using a reverse geocoding API, however, that is beyond the scope of this project\nThank you",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "resume.html#projects",
    "href": "resume.html#projects",
    "title": "Tucker Trost",
    "section": "",
    "text": "Student Records Retention Compliance Dashboard\nOctober 2024 ‚Äì Present\n- Developed a Power BI model that identified over 200,000 student records ready for disposal to comply with AACRAO retention guidelines\n- Wrangled and cleaned data from 5 different databases to implement into the Power BI Model\nPortland Crime Forecasting with XGBoost\nApril 2025\n- Trained an XGBoost model to predict daily Portland, Oregon crime counts with an R¬≤ score of 0.62\n- Applied feature engineering, hyperparameter tuning, and model evaluation with Python to improve prediction accuracy and reveal crime patterns"
  },
  {
    "objectID": "Cleansing_Projects/project1.html#prepare",
    "href": "Cleansing_Projects/project1.html#prepare",
    "title": "Portland Crime Analysis",
    "section": "Prepare",
    "text": "Prepare\n\n\nRead and format project data\n# Load in Data\npcrime_15 = pd.read_csv('CrimeData-2015.csv')\npcrime_16 = pd.read_csv('CrimeData-2016.csv')\npcrime_17 = pd.read_csv('CrimeData-2017.csv')\npcrime_18 = pd.read_csv('CrimeData-2018.csv')\npcrime_19 = pd.read_csv('CrimeData-2019.csv')\npcrime_20 = pd.read_csv('CrimeData-2020.csv')\npcrime_21 = pd.read_csv('CrimeData-2021.csv')\npcrime_22 = pd.read_csv('CrimeData-2022.csv')\npcrime_23 = pd.read_csv('CrimeData-2023.csv')\n\n# Combine Datasets\npcrime_combined = pd.concat([pcrime_15,pcrime_16,pcrime_17,pcrime_18,pcrime_19,pcrime_20,pcrime_21,pcrime_22,pcrime_23], ignore_index=True)\npcrime_combined.head()\n\n\n\n\n\n\n\n\n\n\nAddress\nCaseNumber\nCrimeAgainst\nNeighborhood\nOccurDate\nOccurTime\nOffenseCategory\nOffenseType\nOpenDataLat\nOpenDataLon\nOpenDataX\nOpenDataY\nReportDate\nOffenseCount\n\n\n\n\n0\nNaN\n15-X197430\nPerson\nPiedmont\n5/12/2015\n1400\nAssault Offenses\nIntimidation\nNaN\nNaN\nNaN\nNaN\n5/12/2015\n1\n\n\n1\nNaN\n15-X4282999\nPerson\nBuckman West\n5/1/2015\n2143\nAssault Offenses\nSimple Assault\nNaN\nNaN\nNaN\nNaN\n5/1/2015\n1\n\n\n2\nNaN\n15-X4283033\nPerson\nUniversity Park\n5/1/2015\n1625\nAssault Offenses\nSimple Assault\nNaN\nNaN\nNaN\nNaN\n5/1/2015\n1\n\n\n3\nNaN\n15-X4283218\nPerson\nMadison South\n5/1/2015\n1820\nAssault Offenses\nSimple Assault\nNaN\nNaN\nNaN\nNaN\n5/1/2015\n1\n\n\n4\nNaN\n15-X4283218\nPerson\nMadison South\n5/1/2015\n1820\nKidnapping/Abduction\nKidnapping/Abduction\nNaN\nNaN\nNaN\nNaN\n5/1/2015\n1\n\n\n\n\n\n\n\n\n\nWhat is our goal?\nThinking forward to the analysis that I want to perform with this data, I need to understand what I am looking for when it comes to cleaning. I know that I want to focus my analysis on the distribution of different offense types and categories across the various neighborhoods of Portland. Additionally, I‚Äôd like to get insight into the temporal trends that lie within the data. Based on this understanding, I get a better sense of what aspects of the data need to be cleaned.\n\n\nIdentify Missing Data\npcrime_combined.isna().sum()\n\n\nAddress            44998\nCaseNumber             0\nCrimeAgainst           0\nNeighborhood       17566\nOccurDate              0\nOccurTime              0\nOffenseCategory        0\nOffenseType            0\nOpenDataLat        56511\nOpenDataLon        56511\nOpenDataX          56511\nOpenDataY          56511\nReportDate             0\nOffenseCount           0\ndtype: int64\n\n\n\n\nInitial Observations\n\nA time to report column would be useful\n\nConvert OccurDate and ReportDate to datetime\nCreate a time to report column\n\nOpenDataX/Y don‚Äôt seem necessary for our analysis\n\nDrop OpenDataX and OpenDataY columns\n\nAddress column seems to be redundant as most entries are just a general location\n\nDrop Address column\n\nNeighborhood averages can be used to find lat/lon\n\nDrop rows with missing Neighborhood and OpenDataLat\nReplace all rows with neighborhood but missing Lat/Lon data with average Lat/Lon of their neighborhood",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project1.html#data-cleaning",
    "href": "Cleansing_Projects/project1.html#data-cleaning",
    "title": "Portland Crime Analysis",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\n\nCleaning\n# Calculate average Lat/Lon for each neighborhood\nneighborhood_means = pcrime_combined.groupby('Neighborhood')[['OpenDataLat','OpenDataLon']].transform('mean')\n\n# Clean the data\npcrime_cleaned = (\n    pcrime_combined\n    .drop(columns=['Address', 'OpenDataX', 'OpenDataY'])  # Drop X/Y\n    .dropna(subset=['OpenDataLat', 'Neighborhood'], how='all')  # Drop missing lat/lon and Neighborhoods\n    .assign(\n        OccurDate=pd.to_datetime(pcrime_combined['OccurDate']),  # Convert dates to datetime\n        ReportDate=pd.to_datetime(pcrime_combined['ReportDate']),\n        ReportDiff=lambda x: (x['ReportDate'] - x['OccurDate']).dt.days,  # Calculate time to report\n        OpenDataLat=lambda x: x['OpenDataLat'].fillna(neighborhood_means['OpenDataLat']),  # Fill missing Lat/Lon with average Lat/Lon of given neighborhood\n        OpenDataLon=lambda x: x['OpenDataLon'].fillna(neighborhood_means['OpenDataLon']),\n        OccurTime=lambda x: x['OccurTime'].astype(str).str.zfill(4),  # Ensure time is in HHMM format\n        OccurDateTime=lambda x: pd.to_datetime(\n            x['OccurDate'].dt.strftime('%Y-%m-%d') + ' ' + \n            x['OccurTime'].str[:2] + ':' + x['OccurTime'].str[2:]\n        )  # Combine date and formatted time into datetime\n    )\n    .loc[lambda x: x['OccurDateTime'].dt.year.between(2015, 2023)]  # Filter rows with years within 2015‚Äì2023\n)\n\npcrime_cleaned\n\n\n\n\n\n\n\n\n\n\nCaseNumber\nCrimeAgainst\nNeighborhood\nOccurDate\nOccurTime\nOffenseCategory\nOffenseType\nOpenDataLat\nOpenDataLon\nReportDate\nOffenseCount\nReportDiff\nOccurDateTime\n\n\n\n\n0\n15-X197430\nPerson\nPiedmont\n2015-05-12\n1400\nAssault Offenses\nIntimidation\n45.575321\n-122.669950\n2015-05-12\n1\n0\n2015-05-12 14:00:00\n\n\n1\n15-X4282999\nPerson\nBuckman West\n2015-05-01\n2143\nAssault Offenses\nSimple Assault\n45.517973\n-122.659334\n2015-05-01\n1\n0\n2015-05-01 21:43:00\n\n\n2\n15-X4283033\nPerson\nUniversity Park\n2015-05-01\n1625\nAssault Offenses\nSimple Assault\n45.580393\n-122.727295\n2015-05-01\n1\n0\n2015-05-01 16:25:00\n\n\n3\n15-X4283218\nPerson\nMadison South\n2015-05-01\n1820\nAssault Offenses\nSimple Assault\n45.540839\n-122.578812\n2015-05-01\n1\n0\n2015-05-01 18:20:00\n\n\n4\n15-X4283218\nPerson\nMadison South\n2015-05-01\n1820\nKidnapping/Abduction\nKidnapping/Abduction\n45.540839\n-122.578812\n2015-05-01\n1\n0\n2015-05-01 18:20:00\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n531881\n23-225440\nSociety\nSylvan-Highlands\n2023-08-27\n1420\nWeapon Law Violations\nWeapons Law Violations\n45.508945\n-122.731195\n2023-08-27\n1\n0\n2023-08-27 14:20:00\n\n\n531882\n23-51637\nProperty\nArlington Heights\n2023-02-23\n0330\nMotor Vehicle Theft\nMotor Vehicle Theft\n45.506744\n-122.713355\n2023-02-25\n1\n2\n2023-02-23 03:30:00\n\n\n531883\n23-227984\nPerson\nGoose Hollow\n2023-08-30\n0920\nAssault Offenses\nAggravated Assault\n45.515555\n-122.693709\n2023-08-30\n1\n0\n2023-08-30 09:20:00\n\n\n531884\n23-40689\nProperty\nForest Park\n2023-02-13\n1130\nMotor Vehicle Theft\nMotor Vehicle Theft\n45.540437\n-122.736728\n2023-02-13\n1\n0\n2023-02-13 11:30:00\n\n\n531885\n23-137815\nProperty\nForest Park\n2023-05-23\n1300\nLarceny Offenses\nTheft From Motor Vehicle\n45.540437\n-122.736728\n2023-05-26\n1\n3\n2023-05-23 13:00:00\n\n\n\n\n521247 rows √ó 13 columns\n\n\n\n\nNow we can check and see how we did filling in our missing data.\n\n\nCheck missing data\npcrime_cleaned.isna().sum()\n\n\nCaseNumber            0\nCrimeAgainst          0\nNeighborhood       7870\nOccurDate             0\nOccurTime             0\nOffenseCategory       0\nOffenseType           0\nOpenDataLat           0\nOpenDataLon           0\nReportDate            0\nOffenseCount          0\nReportDiff            0\nOccurDateTime         0\ndtype: int64\n\n\n\nFinal Cleaning Thoughts\nWe have now cleaned our data into a useable state for our analysis. We went from many missing rows from in many columns to only 7881 missing rows in the neighborhood column. Since we have all of the Latitude and Longitude data for each of these missing rows, the missing data will still be useable for our visualizations in Tableau.\nNote: Further cleaning of the missing neighborhood rows could be done using a reverse geocoding API, however, that is beyond the scope of this project",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project1.html#final-thoughts",
    "href": "Cleansing_Projects/project1.html#final-thoughts",
    "title": "Portland Crime Data Cleansing",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nWe have now cleaned our data into a useable state for our analysis. We went from many missing rows from in many columns to only 7881 missing rows in the neighborhood column. Since we have all of the Latitude and Longitude data for each of these missing rows, the missing data will still be useable for our visualizations in Tableau.\nNote: Further cleaning of the missing neighborhood rows could be done using a reverse geocoding API, however, that is beyond the scope of this project\nThank you",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project1.html#elevator-pitch",
    "href": "Cleansing_Projects/project1.html#elevator-pitch",
    "title": "Portland Crime Analysis",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nThis project involves cleaning and exploring crime data from Portland, Oregon (2016-2023). After addressing missing data and unnecessary columns, we uncovered key trends in offense types, report times, and their relationship to neighborhood, day, and time. These insights help provide a clearer picture of crime patterns in the city.\n\nData Source\nThe dateset that I am using was accessed directly from Portland Police Bureau‚Äôs Open Data initiative; compiled from 2015-2023. This dataset is being used under this license.\n\n\nData Dictionary\nAddress: Address of reported incident at the 100 block level (e.g.: 1111 SW 2nd Ave would be 1100 Block SW 2nd Ave).\nCase Number: The case year and number for the reported incident (YY-######).\nCrime Against: Crime against category (Person, Property, or Society).\nNeighborhood: Neighborhood where incident occurred. If the neighborhood name is missing, the incident occurred outside of the boundaries of the Portland neighborhoods or at a location that could not be assigned to a specific address in the system. (e.g., Portland, near Washington Park, on the streetcar, etc.).\nOccur Date: Date the incident occurred. The exact occur date is sometimes unknown. In most situations, the first possible date the crime could have occurred is used as the occur date. (For example, victims return home from a week-long vacation to find their home burglarized. The burglary could have occurred at any point during the week. The first date of their vacation would be listed as the occur date.)\nOccur Time: Time the incident occurred. The exact occur time is sometimes unknown. In most situations, the first possible time the crime could have occurred is used as the occur time. The time is reported in the 24-hour clock format, with the first two digits representing hour (ranges from 00 to 23) and the second two digits representing minutes (ranges from 00 to 59).\nOffense Category: Category of offense (for example, Assault Offenses).\nOffense Type: Type of offense (for example, Aggravated Assault)Note: The statistic for Homicide Offenses has been updated in the Group A Crimes report to align with the 2019 FBI NIBRS definitions. The statistic for Homicide Offenses includes (09A) Murder & Non-negligent Manslaughter and (09B) Negligent Manslaughter. As of January 1, 2019, the FBI expanded the definition of negligent manslaughter to include traffic fatalities that result in an arrest for driving under the influence, distracted driving, or reckless driving. The change in definition impacts the 2019 homicide offenses statistic and the comparability of 2019 homicide statistics to prior year.\nOpen Data Lat/Lon: Generalized Latitude / Longitude of the reported incident. For offenses that occurred at a specific address, the point is mapped to the block‚Äôs midpoint. Offenses that occurred at an intersection is mapped to the intersection centroid.\nOpen Data X/Y: Generalized XY point of the reported incident. For offenses that occurred at a specific address, the point is mapped to the block‚Äôs midpoint. Offenses that occurred at an intersection is mapped to the intersection centroid. To protect the identity of victims and other privacy concerns, the points of certain case types are not released. XY points use the Oregon State Plane North (3601), NAD83 HARN, US International Feet coordinate system.\nOffense Count: Number of offenses per incident. Offenses (i.e.¬†this field) are summed for counting purposes.",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project1.html#my-tableau-dashboard",
    "href": "Cleansing_Projects/project1.html#my-tableau-dashboard",
    "title": "Portland Crime Data Cleansing",
    "section": "My Tableau Dashboard",
    "text": "My Tableau Dashboard\nBelow is an embedded Tableau dashboard:",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "sql.html",
    "href": "sql.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "sql.html#title-2-header",
    "href": "sql.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "SQL/project1.html",
    "href": "SQL/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Projects/project1.html#data-exploration",
    "href": "Cleansing_Projects/project1.html#data-exploration",
    "title": "Portland Crime Analysis",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nThoughts\n\nTemporal Trends\n\nhour/Day/month/year trends\n\nCrime distributions\n\ncategories/types/crimeagainst\nreportdiff by offense type\n\nNeighborhood\n\ntype/category distributions\nreportdiff by neighborhood\noffense count by neighborhood\n\n\n\nTemporal Trends\n\n\nyear-count\npcrime_test = pcrime_cleaned.copy()\n\n# Year Count\npcrime_test['OccurDateTime'] = pd.to_datetime(pcrime_test['OccurDateTime'])\n\nyear_count = pcrime_test.groupby(pcrime_test['OccurDateTime'].dt.year)['OffenseCount'].sum().reset_index()\n\nyear_count.rename(columns={year_count.columns[0]: 'Year'}, inplace=True)\n\nyear_count_fig = ggplot(year_count, aes(x='Year', y='OffenseCount')) + \\\n    geom_line(color='#2e6f40', size=1.5) + \\\n    geom_point(color='#2e6f40', size=3) + \\\n    labs(title='Offense Counts by Year', x='Year', y='Offense Count') + \\\n    theme_minimal2() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10, angle=45), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\nyear_count_fig\n\n\n   \n   \n\n\n\nObservations\nThis chart shows that offense counts remained relatively stable until the pandemic. Starting in 2020, offenses per year surged dramatically through 2022, followed by a sharp decline heading into 2023. This trend underscores the potential impact of the pandemic and lockdowns on crime rates. As the world has gradually returned to normal, it appears crime may be starting to stabilize again.\n\n\nmonth-count\n# Month Count\n\nmonth_count = pcrime_test.groupby(pcrime_test['OccurDateTime'].dt.month)['OffenseCount'].sum().reset_index()\n\nmonth_count.rename(columns={month_count.columns[0]: 'Month'}, inplace=True)\n\nmonth_count_fig = ggplot(month_count, aes(x=\"Month\", y=\"OffenseCount\")) + \\\n    geom_line(color='#2e6f40', size=1.5) + \\\n    geom_point(color='#2e6f40', size=3) + \\\n    labs(title='Offense Counts by Month', x='Month', y='Offense Count') + \\\n    scale_x_continuous(breaks=list(range(1, 13))) + \\\n    theme_minimal2() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\nmonth_count_fig\n\n\n   \n   \n\n\n\n\nObservations\nWhen examining the number of offenses per month, it‚Äôs clear that crime tends to slow down during the winter months and gradually rises through the summer and into the rest of the year. This pattern could be influenced by colder weather during winter, however, maybe bringing in data on weather patterns could shed more light on this trend.\n\n\nweekday_count\n# Weekday Count\nweekday_count = pcrime_test.groupby(pcrime_test['OccurDateTime'].dt.weekday)['OffenseCount'].sum().reset_index()\n\nweekday_count.rename(columns={weekday_count.columns[0]: 'Weekday'}, inplace=True)\n\nweekday_count_fig = ggplot(weekday_count, aes(x=\"Weekday\", y=\"OffenseCount\")) + \\\n    geom_line(color='#2e6f40', size=1.5) + \\\n    geom_point(color='#2e6f40', size=3) + \\\n    labs(title='Offense Counts by Weekday', x='Weekday', y='Offense Count') + \\\n    theme_minimal2() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\nweekday_count_fig\n\n\n   \n   \n\n\n\n\nObservations\nFriday stands out as the day with the most criminal activity. As the start of the weekend, it‚Äôs when many people go out, and with more people out, it‚Äôs likely that more opportunities for crime arise.\n\n\nhour_count\n# Hour Count\nhour_count = pcrime_test.groupby(pcrime_test['OccurDateTime'].dt.hour)['OffenseCount'].sum().reset_index()\n\nhour_count.rename(columns={hour_count.columns[0]: 'Hour'}, inplace=True)\n\nhour_count_fig = ggplot(hour_count, aes(x=\"Hour\", y=\"OffenseCount\")) + \\\n    geom_line(color='#2e6f40', size=1.5) + \\\n    geom_point(color='#2e6f40', size=3) + \\\n    labs(title='Offense Counts by Hour', x='Hour', y='Offense Count') + \\\n    scale_x_continuous(breaks=list(range(0, 25))) + \\\n    theme_minimal2() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\nhour_count_fig\n\n\n   \n   \n\n\n\n\nObservations\nThe trend in offenses by time of day suggests that crime rates may be reactive to human activity. From 1 AM to 7 AM, when most people are asleep, crime rates dip. As the day begins, crime increases, peaking around noon‚Äîpossibly corresponding with lunch breaks‚Äîbefore dropping again when people return to work. Another spike occurs at 5 PM, when many people get off work. Crime rates remain relatively stable throughout the evening, only to surge again at midnight when most people are asleep.\n\n\n\nCrime Distribution Trends\n\n\ncrime_against_count\n# Crime Against Count\ncrime_against = pcrime_test.groupby('CrimeAgainst',as_index=False)['OffenseCount'].sum()\ncrime_against_fig = ggplot(crime_against, aes(x='CrimeAgainst', y='OffenseCount')) + \\\n    geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n    labs(title='Offense Counts by Crime Type', x='Crime Against', y='Offense Count') + \\\n    theme_minimal2() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\ncrime_against_fig\n\n\n   \n   \n\n\n\nObservations\nIt‚Äôs evident that crimes against property are much more common than other categories. My initial thought is that property crimes may be more frequent because they‚Äôre often easier to commit, both physically and morally. Property doesn‚Äôt involve direct harm to individuals, which could make it feel less risky or less severe to potential offenders.\n\n\ncrime_against_report\n# Crime Against Report Time\ncrime_against_report = pcrime_test.groupby('CrimeAgainst',as_index=False)['ReportDiff'].mean()\ncrime_against_report_fig = ggplot(crime_against_report, aes(x='CrimeAgainst', y='ReportDiff')) + \\\n    geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n    labs(title='Average Report Time by Crime Type', x='Crime Against', y='ReportDiff') + \\\n    theme_minimal2() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\ncrime_against_report_fig\n\n\n   \n   \n\n\n\n\nObservations\nThe differences in report time are the most intriguing insight to me. Both crimes against a person and property have an average reporting time of over 6 days after the incident. I‚Äôd love to explore this further to understand the factors that might contribute to this delay and whether there are specific circumstances or patterns influencing the reporting process.\n\n\ncrime_category_count\n# Crime Category Count\ncrime_cat = pcrime_test.groupby('OffenseCategory',as_index=False)['OffenseCount'].sum().sort_values(by='OffenseCount', ascending=False).head(10)\ncrime_cat_fig = ggplot(crime_cat, aes(x='OffenseCount', y='OffenseCategory')) + \\\n    geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n    labs(title='Offense Counts by Category', x='Offense COunt', y='Offense Category') + \\\n    theme_minimal2() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\ncrime_cat_fig\n\n\n   \n   \n\n\n\n\nObservations\nWhen examining the most common offenses by category, larceny far outweighs the others, further reinforcing our findings about the prevalence of property crimes. This aligns with the broader trend of property crimes being more frequent compared to other categories.\n\n\ncrime_category_report\n# Crime Category Report Time\ncrime_cat_report = pcrime_test.groupby('OffenseCategory',as_index=False)['ReportDiff'].mean().sort_values(by='ReportDiff', ascending=False).head(10)\ncrime_cat_report_fig = ggplot(crime_cat_report, aes(x='ReportDiff', y='OffenseCategory')) + \\\n    geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n    labs(title='Average Report Time by Category', x='Average Time to Report (Days)', y='Offense Category') + \\\n    theme_minimal2() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\ncrime_cat_report_fig\n\n\n   \n   \n\n\n\n\nObservation\nOnce again, report time reveals some intriguing insights. The categories with the longest average reporting times tend to be more serious crimes, such as embezzlement, compared to simpler offenses like robbery. This could be due to several factors, including the time it takes for these cases to develop before they are reported. Additionally, there may be a strong emotional aspect, especially in sexual or violent crimes, which could delay the decision to report.\n\n\ncrime_type_report\n# Crime Type Report Time\ncrime_type_report = pcrime_test.groupby('OffenseType',as_index=False)['ReportDiff'].mean().sort_values(by='ReportDiff', ascending=False).head(10)\ncrime_type_report_fig = ggplot(crime_type_report, aes(x='ReportDiff', y='OffenseType')) + \\\n    geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n    labs(title='Average Report Time by Type', x='Average Time to Report (Days)', y='Offense Type') + \\\n    theme_minimal2() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\ncrime_type_report_fig\n\n\n   \n   \n\n\n\n\n\nObservations\nThis chart seems to further confirm my earlier thoughts about report times. It‚Äôs clear that the offense types with the longest average report times are indeed very serious crimes, with almost all of them being sexual in nature. This suggests that the complexity and emotional weight of these crimes could contribute to the delay in reporting.\n\n\ncrime_type_count\n# Crime Type Count\ncrime_type = pcrime_test.groupby('OffenseType',as_index=False)['OffenseCount'].sum().sort_values(by='OffenseCount', ascending=False).head(10)\ncrime_type_fig = ggplot(crime_type, aes(x='OffenseCount', y='OffenseType')) + \\\n    geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n    labs(title='Offense Counts by Type', x='Offense COunt', y='Offense Type') + \\\n    theme_minimal2() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\ncrime_type_fig\n\n\n   \n   \n\n\n\nObservations\nAs seen earlier, larceny offenses dominate the most common crime types. This could be due to the fact that larceny crimes tend to be quick and opportunistic, making them more likely to occur compared to more serious offenses, which often require more time, planning, and emotional involvement.\n\n\n\nNeighborhood Trends\n\n\nneigh_count\n# Neighborhood Count\nneigh_count = pcrime_test.groupby('Neighborhood',as_index=False)['OffenseCount'].sum().sort_values(by='OffenseCount', ascending=False).head(10)\nneigh_count_fig = ggplot(neigh_count,aes(y=neigh_count[\"Neighborhood\"],x=neigh_count[\"OffenseCount\"]))+ \\\n    geom_bar(stat='identity',fill='#2e6f40', color='black')+ \\\n    labs(title='Offense Count by Neighborhood', x='Offense Count', y='Neighborhood') + \\\n    theme_minimal2() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\nneigh_count_fig\n\n\n   \n   \n\n\n\nObservations\nExploring neighborhood trends reveals that Downtown and Hazelwood have disproportionately higher numbers of offenses compared to other areas. However, incorporating data on the size, population, and popularity of these neighborhoods could provide valuable context and help explain these counts more effectively.\n\n\nneigh_report\n# Neighborhood Report Time\nneigh_report = pcrime_test.groupby('Neighborhood',as_index=False)['ReportDiff'].mean().sort_values(by='ReportDiff', ascending=False).head(10)\nneigh_report_fig = ggplot(neigh_report, aes(x='ReportDiff', y='Neighborhood')) + \\\n    geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n    labs(title='Average Report Time by Neighborhood', x='Average Report Time (Days)', y='Neighborhood') + \\\n    theme_minimal2() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\nneigh_report_fig\n\n\n   \n   \n\n\n##### Observations\nThis is an area where I feel more exploration is needed. By diving deeper into the most common offenses in each neighborhood, I suspect it could shed light on why some neighborhoods experience longer reporting times than others.",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project1.html#current-final-observations",
    "href": "Cleansing_Projects/project1.html#current-final-observations",
    "title": "Portland Crime Analysis",
    "section": "Current Final Observations",
    "text": "Current Final Observations\nThis project is still a work in progress, and there‚Äôs plenty more to explore and visualize to better address some of the questions raised during my analysis. That said, the work completed so far has revealed some fascinating insights into crime patterns in Portland, Oregon. Thank you for taking the time to check out my project‚ÄîI hope you‚Äôll return to see the updates as it continues to develop!",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "Cleansing_Projects/pcrime.html#elevator-pitch",
    "href": "Cleansing_Projects/pcrime.html#elevator-pitch",
    "title": "Portland Crime Forecasting with XGBoost",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nThis project involves cleaning, exploring, and modeling crime data from Portland, Oregon (2015‚Äì2023). After addressing missing data and reducing noise, we uncovered key trends in offense types, report times, and their relationship to neighborhood, day, and time. We then developed an XGBoost machine learning model that predicts daily total offense counts by neighborhood, currently achieving an R¬≤ score of 0.62. This model could support better resource planning and crime prevention strategies across the city.\n\nData Source\nThe dateset that I am using was accessed directly from Portland Police Bureau‚Äôs Open Data initiative; compiled from 2015-2023. This dataset is being used under this license.\n\n\nData Dictionary\nAddress: Address of reported incident at the 100 block level (e.g.: 1111 SW 2nd Ave would be 1100 Block SW 2nd Ave).\nCase Number: The case year and number for the reported incident (YY-######).\nCrime Against: Crime against category (Person, Property, or Society).\nNeighborhood: Neighborhood where incident occurred. If the neighborhood name is missing, the incident occurred outside of the boundaries of the Portland neighborhoods or at a location that could not be assigned to a specific address in the system. (e.g., Portland, near Washington Park, on the streetcar, etc.).\nOccur Date: Date the incident occurred. The exact occur date is sometimes unknown. In most situations, the first possible date the crime could have occurred is used as the occur date. (For example, victims return home from a week-long vacation to find their home burglarized. The burglary could have occurred at any point during the week. The first date of their vacation would be listed as the occur date.)\nOccur Time: Time the incident occurred. The exact occur time is sometimes unknown. In most situations, the first possible time the crime could have occurred is used as the occur time. The time is reported in the 24-hour clock format, with the first two digits representing hour (ranges from 00 to 23) and the second two digits representing minutes (ranges from 00 to 59).\nOffense Category: Category of offense (for example, Assault Offenses).\nOffense Type: Type of offense (for example, Aggravated Assault)Note: The statistic for Homicide Offenses has been updated in the Group A Crimes report to align with the 2019 FBI NIBRS definitions. The statistic for Homicide Offenses includes (09A) Murder & Non-negligent Manslaughter and (09B) Negligent Manslaughter. As of January 1, 2019, the FBI expanded the definition of negligent manslaughter to include traffic fatalities that result in an arrest for driving under the influence, distracted driving, or reckless driving. The change in definition impacts the 2019 homicide offenses statistic and the comparability of 2019 homicide statistics to prior year.\nOpen Data Lat/Lon: Generalized Latitude / Longitude of the reported incident. For offenses that occurred at a specific address, the point is mapped to the block‚Äôs midpoint. Offenses that occurred at an intersection is mapped to the intersection centroid.\nOpen Data X/Y: Generalized XY point of the reported incident. For offenses that occurred at a specific address, the point is mapped to the block‚Äôs midpoint. Offenses that occurred at an intersection is mapped to the intersection centroid. To protect the identity of victims and other privacy concerns, the points of certain case types are not released. XY points use the Oregon State Plane North (3601), NAD83 HARN, US International Feet coordinate system.\nOffense Count: Number of offenses per incident. Offenses (i.e.¬†this field) are summed for counting purposes.",
    "crumbs": [
      "Projects",
      "Portland Crime Forecasting with XGBoost"
    ]
  },
  {
    "objectID": "Cleansing_Projects/pcrime.html#prepare",
    "href": "Cleansing_Projects/pcrime.html#prepare",
    "title": "Portland Crime Forecasting with XGBoost",
    "section": "Prepare",
    "text": "Prepare\n\n\nRead and format project data\n# Load in Data\npcrime_15 = pd.read_csv('CrimeData-2015.csv')\npcrime_16 = pd.read_csv('CrimeData-2016.csv')\npcrime_17 = pd.read_csv('CrimeData-2017.csv')\npcrime_18 = pd.read_csv('CrimeData-2018.csv')\npcrime_19 = pd.read_csv('CrimeData-2019.csv')\npcrime_20 = pd.read_csv('CrimeData-2020.csv')\npcrime_21 = pd.read_csv('CrimeData-2021.csv')\npcrime_22 = pd.read_csv('CrimeData-2022.csv')\npcrime_23 = pd.read_csv('CrimeData-2023.csv')\n\n# Combine Datasets\npcrime_combined = pd.concat([pcrime_15,pcrime_16,pcrime_17,pcrime_18,pcrime_19,pcrime_20,pcrime_21,pcrime_22,pcrime_23], ignore_index=True)\npcrime_combined.head()\n\n\n\n\n\n\n\n\n\nAddress\nCaseNumber\nCrimeAgainst\nNeighborhood\nOccurDate\nOccurTime\nOffenseCategory\nOffenseType\nOpenDataLat\nOpenDataLon\nOpenDataX\nOpenDataY\nReportDate\nOffenseCount\n\n\n\n\n0\nNaN\n15-X197430\nPerson\nPiedmont\n5/12/2015\n1400\nAssault Offenses\nIntimidation\nNaN\nNaN\nNaN\nNaN\n5/12/2015\n1\n\n\n1\nNaN\n15-X4282999\nPerson\nBuckman West\n5/1/2015\n2143\nAssault Offenses\nSimple Assault\nNaN\nNaN\nNaN\nNaN\n5/1/2015\n1\n\n\n2\nNaN\n15-X4283033\nPerson\nUniversity Park\n5/1/2015\n1625\nAssault Offenses\nSimple Assault\nNaN\nNaN\nNaN\nNaN\n5/1/2015\n1\n\n\n3\nNaN\n15-X4283218\nPerson\nMadison South\n5/1/2015\n1820\nAssault Offenses\nSimple Assault\nNaN\nNaN\nNaN\nNaN\n5/1/2015\n1\n\n\n4\nNaN\n15-X4283218\nPerson\nMadison South\n5/1/2015\n1820\nKidnapping/Abduction\nKidnapping/Abduction\nNaN\nNaN\nNaN\nNaN\n5/1/2015\n1\n\n\n\n\n\n\n\n\nWhat is our goal?\nThinking forward to the analysis that I want to perform with this data, I need to understand what I am looking for when it comes to cleaning. I know that I want to focus my analysis on the temporal crime trends across the various neighborhoods of Portland. Based on this understanding, I get a better sense of what aspects of the data need to be cleaned.\n\n\nIdentify Missing Data\npcrime_combined.isna().sum()\n\n\nAddress            44998\nCaseNumber             0\nCrimeAgainst           0\nNeighborhood       17566\nOccurDate              0\nOccurTime              0\nOffenseCategory        0\nOffenseType            0\nOpenDataLat        56511\nOpenDataLon        56511\nOpenDataX          56511\nOpenDataY          56511\nReportDate             0\nOffenseCount           0\ndtype: int64\n\n\n\n\nInitial Observations\n\nA time to report column would be useful\n\nConvert OccurDate and ReportDate to datetime\nCreate a time to report column\n\nOpenDataX/Y don‚Äôt seem necessary for our analysis\n\nDrop OpenDataX and OpenDataY columns\n\nAddress column seems to be redundant as most entries are just a general location\n\nDrop Address column\n\nNeighborhood averages can be used to find lat/lon\n\nDrop rows with missing Neighborhood and OpenDataLat\nReplace all rows with neighborhood but missing Lat/Lon data with average Lat/Lon of their neighborhood",
    "crumbs": [
      "Projects",
      "Portland Crime Forecasting with XGBoost"
    ]
  },
  {
    "objectID": "Cleansing_Projects/pcrime.html#data-cleaning",
    "href": "Cleansing_Projects/pcrime.html#data-cleaning",
    "title": "Portland Crime Forecasting with XGBoost",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\n\nCleaning\n# Calculate average Lat/Lon for each neighborhood\nneighborhood_means = pcrime_combined.groupby('Neighborhood')[['OpenDataLat','OpenDataLon']].transform('mean')\n\n# Clean the data\npcrime_cleaned = (\n    pcrime_combined\n    .drop(columns=['Address', 'OpenDataX', 'OpenDataY'])  # Drop X/Y\n    .dropna(subset=['OpenDataLat', 'Neighborhood'], how='all')  # Drop missing lat/lon and Neighborhoods\n    .assign(\n        OccurDate=pd.to_datetime(pcrime_combined['OccurDate']),  # Convert dates to datetime\n        week=lambda x: x.OccurDate.dt.isocalendar().week,\n        year=lambda x: x.OccurDate.dt.year,\n        month=lambda x: x.OccurDate.dt.month,\n        dayofmonth=lambda x: x.OccurDate.dt.day,\n        ReportDate=pd.to_datetime(pcrime_combined['ReportDate']),\n        ReportDiff=lambda x: (x['ReportDate'] - x['OccurDate']).dt.days,  # Calculate time to report\n        OpenDataLat=lambda x: x['OpenDataLat'].fillna(neighborhood_means['OpenDataLat']),  # Fill missing Lat/Lon with average Lat/Lon of given neighborhood\n        OpenDataLon=lambda x: x['OpenDataLon'].fillna(neighborhood_means['OpenDataLon']),\n        OccurTime=lambda x: x['OccurTime'].astype(str).str.zfill(4),  # Ensure time is in HHMM format\n        OccurDateTime=lambda x: pd.to_datetime(\n            x['OccurDate'].dt.strftime('%Y-%m-%d') + ' ' + \n            x['OccurTime'].str[:2] + ':' + x['OccurTime'].str[2:]), # Combine date and formatted time into datetime\n        OccurHour=lambda x: x.OccurDateTime.dt.hour,\n    )\n    .loc[lambda x: x['OccurDateTime'].dt.year.between(2015, 2023)]  # Filter rows with years within 2015‚Äì2023\n)\n\npcrime_cleaned\n\n\n\n\n\n\n\n\n\nCaseNumber\nCrimeAgainst\nNeighborhood\nOccurDate\nOccurTime\nOffenseCategory\nOffenseType\nOpenDataLat\nOpenDataLon\nReportDate\nOffenseCount\nweek\nyear\nmonth\ndayofmonth\nReportDiff\nOccurDateTime\nOccurHour\n\n\n\n\n0\n15-X197430\nPerson\nPiedmont\n2015-05-12\n1400\nAssault Offenses\nIntimidation\n45.575321\n-122.669950\n2015-05-12\n1\n20\n2015\n5\n12\n0\n2015-05-12 14:00:00\n14\n\n\n1\n15-X4282999\nPerson\nBuckman West\n2015-05-01\n2143\nAssault Offenses\nSimple Assault\n45.517973\n-122.659334\n2015-05-01\n1\n18\n2015\n5\n1\n0\n2015-05-01 21:43:00\n21\n\n\n2\n15-X4283033\nPerson\nUniversity Park\n2015-05-01\n1625\nAssault Offenses\nSimple Assault\n45.580393\n-122.727295\n2015-05-01\n1\n18\n2015\n5\n1\n0\n2015-05-01 16:25:00\n16\n\n\n3\n15-X4283218\nPerson\nMadison South\n2015-05-01\n1820\nAssault Offenses\nSimple Assault\n45.540839\n-122.578812\n2015-05-01\n1\n18\n2015\n5\n1\n0\n2015-05-01 18:20:00\n18\n\n\n4\n15-X4283218\nPerson\nMadison South\n2015-05-01\n1820\nKidnapping/Abduction\nKidnapping/Abduction\n45.540839\n-122.578812\n2015-05-01\n1\n18\n2015\n5\n1\n0\n2015-05-01 18:20:00\n18\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n531881\n23-225440\nSociety\nSylvan-Highlands\n2023-08-27\n1420\nWeapon Law Violations\nWeapons Law Violations\n45.508945\n-122.731195\n2023-08-27\n1\n34\n2023\n8\n27\n0\n2023-08-27 14:20:00\n14\n\n\n531882\n23-51637\nProperty\nArlington Heights\n2023-02-23\n0330\nMotor Vehicle Theft\nMotor Vehicle Theft\n45.506744\n-122.713355\n2023-02-25\n1\n8\n2023\n2\n23\n2\n2023-02-23 03:30:00\n3\n\n\n531883\n23-227984\nPerson\nGoose Hollow\n2023-08-30\n0920\nAssault Offenses\nAggravated Assault\n45.515555\n-122.693709\n2023-08-30\n1\n35\n2023\n8\n30\n0\n2023-08-30 09:20:00\n9\n\n\n531884\n23-40689\nProperty\nForest Park\n2023-02-13\n1130\nMotor Vehicle Theft\nMotor Vehicle Theft\n45.540437\n-122.736728\n2023-02-13\n1\n7\n2023\n2\n13\n0\n2023-02-13 11:30:00\n11\n\n\n531885\n23-137815\nProperty\nForest Park\n2023-05-23\n1300\nLarceny Offenses\nTheft From Motor Vehicle\n45.540437\n-122.736728\n2023-05-26\n1\n21\n2023\n5\n23\n3\n2023-05-23 13:00:00\n13\n\n\n\n\n521247 rows √ó 18 columns\n\n\n\n\n\nShow the code\npcrime_cleaned.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 521247 entries, 0 to 531885\nData columns (total 18 columns):\n #   Column           Non-Null Count   Dtype         \n---  ------           --------------   -----         \n 0   CaseNumber       521247 non-null  object        \n 1   CrimeAgainst     521247 non-null  object        \n 2   Neighborhood     513377 non-null  object        \n 3   OccurDate        521247 non-null  datetime64[ns]\n 4   OccurTime        521247 non-null  object        \n 5   OffenseCategory  521247 non-null  object        \n 6   OffenseType      521247 non-null  object        \n 7   OpenDataLat      521247 non-null  float64       \n 8   OpenDataLon      521247 non-null  float64       \n 9   ReportDate       521247 non-null  datetime64[ns]\n 10  OffenseCount     521247 non-null  int64         \n 11  week             521247 non-null  UInt32        \n 12  year             521247 non-null  int32         \n 13  month            521247 non-null  int32         \n 14  dayofmonth       521247 non-null  int32         \n 15  ReportDiff       521247 non-null  int64         \n 16  OccurDateTime    521247 non-null  datetime64[ns]\n 17  OccurHour        521247 non-null  int32         \ndtypes: UInt32(1), datetime64[ns](3), float64(2), int32(4), int64(2), object(6)\nmemory usage: 66.1+ MB\n\n\nNow we can check and see how we did filling in our missing data.\n\n\nCheck missing data\npcrime_cleaned.isna().sum()\n\n\nCaseNumber            0\nCrimeAgainst          0\nNeighborhood       7870\nOccurDate             0\nOccurTime             0\nOffenseCategory       0\nOffenseType           0\nOpenDataLat           0\nOpenDataLon           0\nReportDate            0\nOffenseCount          0\nweek                  0\nyear                  0\nmonth                 0\ndayofmonth            0\nReportDiff            0\nOccurDateTime         0\nOccurHour             0\ndtype: int64\n\n\n\nFinal Cleaning Thoughts\nWe have now cleaned our data into a useable state for our analysis. We went from many missing rows from in many columns to only 7881 missing rows in the neighborhood column.\nNote: Further cleaning of the missing neighborhood rows could be done using a reverse geocoding API, however, that is beyond the scope of this project",
    "crumbs": [
      "Projects",
      "Portland Crime Forecasting with XGBoost"
    ]
  },
  {
    "objectID": "Cleansing_Projects/pcrime.html#data-exploration",
    "href": "Cleansing_Projects/pcrime.html#data-exploration",
    "title": "Portland Crime Forecasting with XGBoost",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nThoughts\n\nTemporal Trends\n\nhour/Day/month/year trends\n\nCrime distributions\n\ncategories/types/crimeagainst\nreportdiff by offense type\n\nNeighborhood\n\ntype/category distributions\nreportdiff by neighborhood\noffense count by neighborhood\n\n\n\nTemporal Trends\n\n\nYear Count\nfrom lets_plot import *\nLetsPlot.setup_html()\nmonth_year = (pcrime_cleaned\n    .assign(\n        year=pcrime_cleaned['OccurDateTime'].dt.year,\n        month=pcrime_cleaned['OccurDateTime'].dt.month) \n    .query('year &gt;= 2019')\\\n    .groupby(['year', 'month'])\\\n    ['OffenseCount'].sum()\\\n    .reset_index()\\\n    .rename(columns={'OffenseCount': 'count'})\n)\n\nlast_month_data = (\n    month_year.groupby(\"year\")\n    .apply(lambda df: df[df[\"month\"] == df[\"month\"].max()])\n    .reset_index(drop=True)\n)\n\nmonth_year[\"year\"] = month_year[\"year\"].astype(str)  # Convert year to string\nlast_month_data[\"year\"] = last_month_data[\"year\"].astype(str)  # Convert for labels\n\nmonth_year_fig = (\n    ggplot(month_year, aes(x=\"month\", y=\"count\", color=\"year\", group=\"year\")) +\n    geom_smooth(method='loess', span=0.5,se=False) +\n    geom_label(data=last_month_data, mapping=aes(y='count',label=\"year\", color=\"year\"),x=12.5, size=7,check_overlap=True)+\n    labs(title=\"Offense Counts by Month and Year\", x=\"Month\", y=\"Offense Count\") +\n    theme_minimal() +\n    theme(legend_position=\"none\") +\n    scale_x_continuous(breaks=list(range(1, 13)))+\n    scale_y_continuous(breaks=[4500,4750,5000,5250,5500,5750,6000,6250,6500]))\n\n\nmonth_year_fig\n\n\n\n            \n            \n            \n\n\n   \n   \n\n\n\nObservations\nThis chart shows that crime levels remained fairly consistent in 2019. In 2020, we see a noticeable drop when the country went into lockdown, followed by a sharp increase as restrictions eased in the summer. Then, in 2021, Portland experienced a significant surge in crime, which remained relatively high until 2023, when it began to stabilize.\n\n\nOther Temporal Counts\n# Month Count\n\nmonth_count = pcrime_cleaned.groupby(pcrime_cleaned['OccurDateTime'].dt.month)['OffenseCount'].sum().reset_index()\n\nmonth_count.rename(columns={month_count.columns[0]: 'Month'}, inplace=True)\n\nmonth_count_fig = ggplot(month_count, aes(x=\"Month\", y=\"OffenseCount\")) + \\\n    geom_line(color='#2e6f40', size=1.5) + \\\n    geom_point(color='#2e6f40', size=3) + \\\n    labs(title='Offense Counts by Month', x='Month', y='Offense Count') + \\\n    scale_x_continuous(breaks=list(range(1, 13))) + \\\n    theme_minimal() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\n# Weekday Count\nweekday_count = pcrime_cleaned.groupby(pcrime_cleaned['OccurDateTime'].dt.weekday)['OffenseCount'].sum().reset_index()\n\nweekday_count.rename(columns={weekday_count.columns[0]: 'Weekday'}, inplace=True)\n\nweekday_count_fig = ggplot(weekday_count, aes(x=\"Weekday\", y=\"OffenseCount\")) + \\\n    geom_line(color='#2e6f40', size=1.5) + \\\n    geom_point(color='#2e6f40', size=3) + \\\n    labs(title='Offense Counts by Weekday', x='Weekday', y='Offense Count') + \\\n    theme_minimal() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\n# Hour Count\nhour_count = pcrime_cleaned.groupby(pcrime_cleaned['OccurDateTime'].dt.hour)['OffenseCount'].sum().reset_index()\n\nhour_count.rename(columns={hour_count.columns[0]: 'Hour'}, inplace=True)\n\nhour_count_fig = ggplot(hour_count, aes(x=\"Hour\", y=\"OffenseCount\")) + \\\n    geom_line(color='#2e6f40', size=1.5) + \\\n    geom_point(color='#2e6f40', size=3) + \\\n    labs(title='Offense Counts by Hour', x='Hour', y='Offense Count') + \\\n    scale_x_continuous(breaks=list(range(0, 25))) + \\\n    theme_minimal() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\n# Create a list of plots to display in a single row\ntemp_plot_list = [\n    month_count_fig,\n    weekday_count_fig,\n    hour_count_fig\n]\n\n# Arrange the plots in a single row\ntemp_plots = gggrid(temp_plot_list, ncol=3) + ggsize(1200, 400)\n\n# Show the combined plot\ntemp_plots\n\n\n   \n   \n\n\n\n\nObservations\nCrime patterns exhibit distinct temporal trends across months, weekdays, and hours. Monthly data shows that crime tends to slow down during the winter months and gradually rises through the summer and into the rest of the year, potentially influenced by seasonal factors such as weather and increased outdoor activity. Looking at weekly patterns, Friday stands out as the day with the highest number of reported offenses, which aligns with the start of the weekend when more people are out, creating more opportunities for crime. Additionally, crime follows a predictable daily cycle, with certain hours experiencing higher offense counts. These trends suggest that external factors like weather, social behavior, and law enforcement presence may play a role in crime fluctuations, warranting further analysis to uncover deeper insights.\n\n\n\nCrime Distribution Trends\n\n\nCrime Against\n# Crime Against Count\ncrime_against = pcrime_cleaned.groupby('CrimeAgainst',as_index=False)['OffenseCount'].sum()\ncrime_against_fig = ggplot(crime_against, aes(x='CrimeAgainst', y='OffenseCount')) + \\\n    geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n    labs(title='Offense Counts by Crime Type', x='Crime Against', y='Offense Count') + \\\n    theme_minimal() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\n# Crime Against Report Time\n\ncrime_against_report_box = (ggplot(pcrime_cleaned,aes(x='ReportDiff',y='CrimeAgainst'))+\n  geom_boxplot(outlier_shape = None,fill='#2e6f40', color='black')+\n  scale_x_log10()+\n  theme_minimal()+\n  labs(title = 'Crime Against Report Time Distribution',x= 'Days to Report',y='Crime Against'))\n\ncrime_against_list = [crime_against_fig,crime_against_report_box]\n\ncrime_against_plots = gggrid(crime_against_list,ncol=2)+ ggsize(1600, 600)\n\ncrime_against_plots\n\n\n   \n   \n\n\n\nObservations\nIt‚Äôs evident that crimes against property are much more common than other categories. My initial thought is that property crimes may be more frequent because they‚Äôre often easier to commit, both physically and morally. Property doesn‚Äôt involve direct harm to individuals, which could make it feel less risky or less severe to potential offenders. The differences in report times are also interesting. All crime against types have a median report time of 1 day, however, crime against person has a larger distribution of report times. This also makes sense because many crimes against a person are very sensitive situations that lead to delayed reporting.\n\n\nCrime Category & Type\n# Crime Category Count\ncrime_cat = pcrime_cleaned.groupby('OffenseCategory',as_index=False)['OffenseCount'].sum().sort_values(by='OffenseCount', ascending=False).head(10)\ncrime_cat_fig = ggplot(crime_cat, aes(x='OffenseCount', y='OffenseCategory')) + \\\n    geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n    labs(title='Offense Counts by Category', x='Offense COunt', y='Offense Category') + \\\n    theme_minimal() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\ncrime_cat_fig\n\n# Crime Type Count\ncrime_type = pcrime_cleaned.groupby('OffenseType',as_index=False)['OffenseCount'].sum().sort_values(by='OffenseCount', ascending=False).head(10)\ncrime_type_fig = ggplot(crime_type, aes(x='OffenseCount', y='OffenseType')) + \\\n    geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n    labs(title='Offense Counts by Type', x='Offense COunt', y='Offense Type') + \\\n    theme_minimal() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\ncrime_type_fig\n\n# Neighborhood Count\nneigh_count = pcrime_cleaned.groupby('Neighborhood',as_index=False)['OffenseCount'].sum().sort_values(by='OffenseCount', ascending=False).head(10)\nneigh_count_fig = ggplot(neigh_count,aes(y=neigh_count[\"Neighborhood\"],x=neigh_count[\"OffenseCount\"]))+ \\\n    geom_bar(stat='identity',fill='#2e6f40', color='black')+ \\\n    labs(title='Offense Count by Neighborhood', x='Offense Count', y='Neighborhood') + \\\n    theme_minimal() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\nneigh_count_fig\n\ncount_list = [crime_cat_fig,crime_type_fig,neigh_count_fig]\n\ncount_plots = gggrid(count_list, ncol=2) + ggsize(1200, 400)\n\ncount_plots\n\n\n   \n   \n\n\n\n\nObservations\nLarceny stands out as the most common offense in Portland, reinforcing the broader trend that property crimes are significantly more prevalent than other crime categories. This may be attributed to the opportunistic nature of larceny‚Äîthese offenses often require little planning and can happen quickly, unlike more serious crimes that demand time, effort, or emotional involvement. Additionally, neighborhood-level analysis shows that Downtown and Hazelwood experience disproportionately high numbers of reported offenses. While this highlights potential crime hotspots, further context‚Äîsuch as population density, neighborhood size, and visitor traffic‚Äîwould provide a more accurate understanding of these patterns.",
    "crumbs": [
      "Projects",
      "Portland Crime Forecasting with XGBoost"
    ]
  },
  {
    "objectID": "Cleansing_Projects/pcrime.html#current-final-observations",
    "href": "Cleansing_Projects/pcrime.html#current-final-observations",
    "title": "Portland Crime Analysis",
    "section": "Current Final Observations",
    "text": "Current Final Observations\nThis project is still a work in progress, and there‚Äôs plenty more to explore and visualize to better address some of the questions raised during my analysis. That said, the work completed so far has revealed some fascinating insights into crime patterns in Portland, Oregon. Thank you for taking the time to check out my project‚ÄîI hope you‚Äôll return to see the updates as it continues to develop!",
    "crumbs": [
      "Projects",
      "Portland Crime Analysis"
    ]
  },
  {
    "objectID": "letsplot_env/lib/python3.12/site-packages/pandas/tests/indexes/datetimes/test_indexing.html",
    "href": "letsplot_env/lib/python3.12/site-packages/pandas/tests/indexes/datetimes/test_indexing.html",
    "title": "Tucker Trost - Data Science Portfolio",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "letsplot_env/lib/python3.12/site-packages/pandas/tests/indexes/period/test_constructors.html",
    "href": "letsplot_env/lib/python3.12/site-packages/pandas/tests/indexes/period/test_constructors.html",
    "title": "Tucker Trost - Data Science Portfolio",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "letsplot_env/lib/python3.12/site-packages/httpcore-1.0.7.dist-info/licenses/LICENSE.html",
    "href": "letsplot_env/lib/python3.12/site-packages/httpcore-1.0.7.dist-info/licenses/LICENSE.html",
    "title": "Tucker Trost - Data Science Portfolio",
    "section": "",
    "text": "Copyright ¬© 2020, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ‚ÄúAS IS‚Äù AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n Back to top"
  },
  {
    "objectID": "letsplot_env/lib/python3.12/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "href": "letsplot_env/lib/python3.12/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "title": "Tucker Trost - Data Science Portfolio",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ‚ÄúAS IS‚Äù AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n Back to top"
  },
  {
    "objectID": "letsplot_env/lib/python3.12/site-packages/soupsieve-2.6.dist-info/licenses/LICENSE.html",
    "href": "letsplot_env/lib/python3.12/site-packages/soupsieve-2.6.dist-info/licenses/LICENSE.html",
    "title": "Tucker Trost - Data Science Portfolio",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 - 2024 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n Back to top"
  },
  {
    "objectID": "letsplot_env/lib/python3.12/site-packages/idna-3.10.dist-info/LICENSE.html",
    "href": "letsplot_env/lib/python3.12/site-packages/idna-3.10.dist-info/LICENSE.html",
    "title": "Tucker Trost - Data Science Portfolio",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ‚ÄúAS IS‚Äù AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n Back to top"
  },
  {
    "objectID": "letsplot_env/lib/python3.12/site-packages/cffi/recompiler.html",
    "href": "letsplot_env/lib/python3.12/site-packages/cffi/recompiler.html",
    "title": "Tucker Trost - Data Science Portfolio",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "letsplot_env/lib/python3.12/site-packages/numpy/random/LICENSE.html",
    "href": "letsplot_env/lib/python3.12/site-packages/numpy/random/LICENSE.html",
    "title": "NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ‚ÄúAS IS‚Äù AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm‚Äôs designer. Component licenses are located with the component code.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "letsplot_env/lib/python3.12/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "href": "letsplot_env/lib/python3.12/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "title": "Tucker Trost - Data Science Portfolio",
    "section": "",
    "text": "Copyright ¬© 2019, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ‚ÄúAS IS‚Äù AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n Back to top"
  },
  {
    "objectID": "letsplot_env/lib/python3.12/site-packages/pandas/tests/indexes/period/test_indexing.html",
    "href": "letsplot_env/lib/python3.12/site-packages/pandas/tests/indexes/period/test_indexing.html",
    "title": "Tucker Trost - Data Science Portfolio",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "Cleansing_Projects/pcrime copy.html#elevator-pitch",
    "href": "Cleansing_Projects/pcrime copy.html#elevator-pitch",
    "title": "Portland Crime Analysis",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nThis project involves cleaning and exploring crime data from Portland, Oregon (2016-2023). After addressing missing data and unnecessary columns, we uncovered key trends in offense types, report times, and their relationship to neighborhood, day, and time. These insights help provide a clearer picture of crime patterns in the city.\n\nData Source\nThe dateset that I am using was accessed directly from Portland Police Bureau‚Äôs Open Data initiative; compiled from 2015-2023. This dataset is being used under this license.\n\n\nData Dictionary\nAddress: Address of reported incident at the 100 block level (e.g.: 1111 SW 2nd Ave would be 1100 Block SW 2nd Ave).\nCase Number: The case year and number for the reported incident (YY-######).\nCrime Against: Crime against category (Person, Property, or Society).\nNeighborhood: Neighborhood where incident occurred. If the neighborhood name is missing, the incident occurred outside of the boundaries of the Portland neighborhoods or at a location that could not be assigned to a specific address in the system. (e.g., Portland, near Washington Park, on the streetcar, etc.).\nOccur Date: Date the incident occurred. The exact occur date is sometimes unknown. In most situations, the first possible date the crime could have occurred is used as the occur date. (For example, victims return home from a week-long vacation to find their home burglarized. The burglary could have occurred at any point during the week. The first date of their vacation would be listed as the occur date.)\nOccur Time: Time the incident occurred. The exact occur time is sometimes unknown. In most situations, the first possible time the crime could have occurred is used as the occur time. The time is reported in the 24-hour clock format, with the first two digits representing hour (ranges from 00 to 23) and the second two digits representing minutes (ranges from 00 to 59).\nOffense Category: Category of offense (for example, Assault Offenses).\nOffense Type: Type of offense (for example, Aggravated Assault)Note: The statistic for Homicide Offenses has been updated in the Group A Crimes report to align with the 2019 FBI NIBRS definitions. The statistic for Homicide Offenses includes (09A) Murder & Non-negligent Manslaughter and (09B) Negligent Manslaughter. As of January 1, 2019, the FBI expanded the definition of negligent manslaughter to include traffic fatalities that result in an arrest for driving under the influence, distracted driving, or reckless driving. The change in definition impacts the 2019 homicide offenses statistic and the comparability of 2019 homicide statistics to prior year.\nOpen Data Lat/Lon: Generalized Latitude / Longitude of the reported incident. For offenses that occurred at a specific address, the point is mapped to the block‚Äôs midpoint. Offenses that occurred at an intersection is mapped to the intersection centroid.\nOpen Data X/Y: Generalized XY point of the reported incident. For offenses that occurred at a specific address, the point is mapped to the block‚Äôs midpoint. Offenses that occurred at an intersection is mapped to the intersection centroid. To protect the identity of victims and other privacy concerns, the points of certain case types are not released. XY points use the Oregon State Plane North (3601), NAD83 HARN, US International Feet coordinate system.\nOffense Count: Number of offenses per incident. Offenses (i.e.¬†this field) are summed for counting purposes."
  },
  {
    "objectID": "Cleansing_Projects/pcrime copy.html#prepare",
    "href": "Cleansing_Projects/pcrime copy.html#prepare",
    "title": "Portland Crime Analysis",
    "section": "Prepare",
    "text": "Prepare\n\n\nRead and format project data\n# Load in Data\npcrime_15 = pd.read_csv('CrimeData-2015.csv')\npcrime_16 = pd.read_csv('CrimeData-2016.csv')\npcrime_17 = pd.read_csv('CrimeData-2017.csv')\npcrime_18 = pd.read_csv('CrimeData-2018.csv')\npcrime_19 = pd.read_csv('CrimeData-2019.csv')\npcrime_20 = pd.read_csv('CrimeData-2020.csv')\npcrime_21 = pd.read_csv('CrimeData-2021.csv')\npcrime_22 = pd.read_csv('CrimeData-2022.csv')\npcrime_23 = pd.read_csv('CrimeData-2023.csv')\n\n# Combine Datasets\npcrime_combined = pd.concat([pcrime_15,pcrime_16,pcrime_17,pcrime_18,pcrime_19,pcrime_20,pcrime_21,pcrime_22,pcrime_23], ignore_index=True)\npcrime_combined.head()\n\n\n\n\n\n\n\n\n\nAddress\nCaseNumber\nCrimeAgainst\nNeighborhood\nOccurDate\nOccurTime\nOffenseCategory\nOffenseType\nOpenDataLat\nOpenDataLon\nOpenDataX\nOpenDataY\nReportDate\nOffenseCount\n\n\n\n\n0\nNaN\n15-X197430\nPerson\nPiedmont\n5/12/2015\n1400\nAssault Offenses\nIntimidation\nNaN\nNaN\nNaN\nNaN\n5/12/2015\n1\n\n\n1\nNaN\n15-X4282999\nPerson\nBuckman West\n5/1/2015\n2143\nAssault Offenses\nSimple Assault\nNaN\nNaN\nNaN\nNaN\n5/1/2015\n1\n\n\n2\nNaN\n15-X4283033\nPerson\nUniversity Park\n5/1/2015\n1625\nAssault Offenses\nSimple Assault\nNaN\nNaN\nNaN\nNaN\n5/1/2015\n1\n\n\n3\nNaN\n15-X4283218\nPerson\nMadison South\n5/1/2015\n1820\nAssault Offenses\nSimple Assault\nNaN\nNaN\nNaN\nNaN\n5/1/2015\n1\n\n\n4\nNaN\n15-X4283218\nPerson\nMadison South\n5/1/2015\n1820\nKidnapping/Abduction\nKidnapping/Abduction\nNaN\nNaN\nNaN\nNaN\n5/1/2015\n1\n\n\n\n\n\n\n\n\nWhat is our goal?\nThinking forward to the analysis that I want to perform with this data, I need to understand what I am looking for when it comes to cleaning. I know that I want to focus my analysis on the distribution of different offense types and categories across the various neighborhoods of Portland. Additionally, I‚Äôd like to get insight into the temporal trends that lie within the data. Based on this understanding, I get a better sense of what aspects of the data need to be cleaned.\n\n\nIdentify Missing Data\npcrime_combined.isna().sum()\n\n\nAddress            44998\nCaseNumber             0\nCrimeAgainst           0\nNeighborhood       17566\nOccurDate              0\nOccurTime              0\nOffenseCategory        0\nOffenseType            0\nOpenDataLat        56511\nOpenDataLon        56511\nOpenDataX          56511\nOpenDataY          56511\nReportDate             0\nOffenseCount           0\ndtype: int64\n\n\n\n\nInitial Observations\n\nA time to report column would be useful\n\nConvert OccurDate and ReportDate to datetime\nCreate a time to report column\n\nOpenDataX/Y don‚Äôt seem necessary for our analysis\n\nDrop OpenDataX and OpenDataY columns\n\nAddress column seems to be redundant as most entries are just a general location\n\nDrop Address column\n\nNeighborhood averages can be used to find lat/lon\n\nDrop rows with missing Neighborhood and OpenDataLat\nReplace all rows with neighborhood but missing Lat/Lon data with average Lat/Lon of their neighborhood"
  },
  {
    "objectID": "Cleansing_Projects/pcrime copy.html#data-cleaning",
    "href": "Cleansing_Projects/pcrime copy.html#data-cleaning",
    "title": "Portland Crime Analysis",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\n\nCleaning\n# Calculate average Lat/Lon for each neighborhood\nneighborhood_means = pcrime_combined.groupby('Neighborhood')[['OpenDataLat','OpenDataLon']].transform('mean')\n\n# Clean the data\npcrime_cleaned = (\n    pcrime_combined\n    .drop(columns=['Address', 'OpenDataX', 'OpenDataY'])  # Drop X/Y\n    .dropna(subset=['OpenDataLat', 'Neighborhood'], how='all')  # Drop missing lat/lon and Neighborhoods\n    .assign(\n        OccurDate=pd.to_datetime(pcrime_combined['OccurDate']),  # Convert dates to datetime\n        ReportDate=pd.to_datetime(pcrime_combined['ReportDate']),\n        ReportDiff=lambda x: (x['ReportDate'] - x['OccurDate']).dt.days,  # Calculate time to report\n        OpenDataLat=lambda x: x['OpenDataLat'].fillna(neighborhood_means['OpenDataLat']),  # Fill missing Lat/Lon with average Lat/Lon of given neighborhood\n        OpenDataLon=lambda x: x['OpenDataLon'].fillna(neighborhood_means['OpenDataLon']),\n        OccurTime=lambda x: x['OccurTime'].astype(str).str.zfill(4),  # Ensure time is in HHMM format\n        OccurDateTime=lambda x: pd.to_datetime(\n            x['OccurDate'].dt.strftime('%Y-%m-%d') + ' ' + \n            x['OccurTime'].str[:2] + ':' + x['OccurTime'].str[2:]\n        )  # Combine date and formatted time into datetime\n    )\n    .loc[lambda x: x['OccurDateTime'].dt.year.between(2015, 2023)]  # Filter rows with years within 2015‚Äì2023\n)\n\npcrime_cleaned\n\n\n\n\n\n\n\n\n\nCaseNumber\nCrimeAgainst\nNeighborhood\nOccurDate\nOccurTime\nOffenseCategory\nOffenseType\nOpenDataLat\nOpenDataLon\nReportDate\nOffenseCount\nReportDiff\nOccurDateTime\n\n\n\n\n0\n15-X197430\nPerson\nPiedmont\n2015-05-12\n1400\nAssault Offenses\nIntimidation\n45.575321\n-122.669950\n2015-05-12\n1\n0\n2015-05-12 14:00:00\n\n\n1\n15-X4282999\nPerson\nBuckman West\n2015-05-01\n2143\nAssault Offenses\nSimple Assault\n45.517973\n-122.659334\n2015-05-01\n1\n0\n2015-05-01 21:43:00\n\n\n2\n15-X4283033\nPerson\nUniversity Park\n2015-05-01\n1625\nAssault Offenses\nSimple Assault\n45.580393\n-122.727295\n2015-05-01\n1\n0\n2015-05-01 16:25:00\n\n\n3\n15-X4283218\nPerson\nMadison South\n2015-05-01\n1820\nAssault Offenses\nSimple Assault\n45.540839\n-122.578812\n2015-05-01\n1\n0\n2015-05-01 18:20:00\n\n\n4\n15-X4283218\nPerson\nMadison South\n2015-05-01\n1820\nKidnapping/Abduction\nKidnapping/Abduction\n45.540839\n-122.578812\n2015-05-01\n1\n0\n2015-05-01 18:20:00\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n531881\n23-225440\nSociety\nSylvan-Highlands\n2023-08-27\n1420\nWeapon Law Violations\nWeapons Law Violations\n45.508945\n-122.731195\n2023-08-27\n1\n0\n2023-08-27 14:20:00\n\n\n531882\n23-51637\nProperty\nArlington Heights\n2023-02-23\n0330\nMotor Vehicle Theft\nMotor Vehicle Theft\n45.506744\n-122.713355\n2023-02-25\n1\n2\n2023-02-23 03:30:00\n\n\n531883\n23-227984\nPerson\nGoose Hollow\n2023-08-30\n0920\nAssault Offenses\nAggravated Assault\n45.515555\n-122.693709\n2023-08-30\n1\n0\n2023-08-30 09:20:00\n\n\n531884\n23-40689\nProperty\nForest Park\n2023-02-13\n1130\nMotor Vehicle Theft\nMotor Vehicle Theft\n45.540437\n-122.736728\n2023-02-13\n1\n0\n2023-02-13 11:30:00\n\n\n531885\n23-137815\nProperty\nForest Park\n2023-05-23\n1300\nLarceny Offenses\nTheft From Motor Vehicle\n45.540437\n-122.736728\n2023-05-26\n1\n3\n2023-05-23 13:00:00\n\n\n\n\n521247 rows √ó 13 columns\n\n\n\nNow we can check and see how we did filling in our missing data.\n\n\nCheck missing data\npcrime_cleaned.isna().sum()\n\n\nCaseNumber            0\nCrimeAgainst          0\nNeighborhood       7870\nOccurDate             0\nOccurTime             0\nOffenseCategory       0\nOffenseType           0\nOpenDataLat           0\nOpenDataLon           0\nReportDate            0\nOffenseCount          0\nReportDiff            0\nOccurDateTime         0\ndtype: int64\n\n\n\nFinal Cleaning Thoughts\nWe have now cleaned our data into a useable state for our analysis. We went from many missing rows from in many columns to only 7881 missing rows in the neighborhood column. Since we have all of the Latitude and Longitude data for each of these missing rows, the missing data will still be useable for our visualizations in Tableau.\nNote: Further cleaning of the missing neighborhood rows could be done using a reverse geocoding API, however, that is beyond the scope of this project"
  },
  {
    "objectID": "Cleansing_Projects/pcrime copy.html#data-exploration",
    "href": "Cleansing_Projects/pcrime copy.html#data-exploration",
    "title": "Portland Crime Analysis",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nThoughts\n\nTemporal Trends\n\nhour/Day/month/year trends\n\nCrime distributions\n\ncategories/types/crimeagainst\nreportdiff by offense type\n\nNeighborhood\n\ntype/category distributions\nreportdiff by neighborhood\noffense count by neighborhood\n\n\n\nTemporal Trends\n\n\nyear-count\nfrom lets_plot import *\nLetsPlot.setup_html()\nmonth_year = (pcrime_cleaned\n    .assign(\n        year=pcrime_cleaned['OccurDateTime'].dt.year,\n        month=pcrime_cleaned['OccurDateTime'].dt.month) \n    .query('year &gt;= 2019')\\\n    .groupby(['year', 'month'])\\\n    ['OffenseCount'].sum()\\\n    .reset_index()\\\n    .rename(columns={'OffenseCount': 'count'})\n)\n\nlast_month_data = (\n    month_year.groupby(\"year\")\n    .apply(lambda df: df[df[\"month\"] == df[\"month\"].max()])\n    .reset_index(drop=True)\n)\n\nmonth_year[\"year\"] = month_year[\"year\"].astype(str)  # Convert year to string\nlast_month_data[\"year\"] = last_month_data[\"year\"].astype(str)  # Convert for labels\n\nmonth_year_fig = (\n    ggplot(month_year, aes(x=\"month\", y=\"count\", color=\"year\", group=\"year\")) +\n    geom_smooth(method='loess', span=0.5,se=False) +\n    geom_label(data=last_month_data, mapping=aes(y='count',label=\"year\", color=\"year\"),x=12.5, size=7,check_overlap=True)\n +  # Equivalent of geom_label_repel\n    labs(title=\"Offense Counts by Month and Year\", x=\"Month\", y=\"Offense Count\") +\n    theme_minimal() +\n    theme(legend_position=\"none\") +\n    scale_x_continuous(breaks=list(range(1, 13)))+\n    scale_y_continuous(breaks=[4500,4750,5000,5250,5500,5750,6000,6250,6500]))\n\n\nmonth_year_fig\n\n\n\n            \n            \n            \n\n\n   \n   \n\n\n\nObservations\nThis chart shows that crime levels remained fairly consistent in 2019. In 2020, we see a noticeable drop when the country went into lockdown, followed by a sharp increase as restrictions eased in the summer. Then, in 2021, Portland experienced a significant surge in crime, which remained relatively high until 2023, when it began to stabilize.\n\n\nTemporal Counts\n# Month Count\n\nmonth_count = pcrime_cleaned.groupby(pcrime_cleaned['OccurDateTime'].dt.month)['OffenseCount'].sum().reset_index()\n\nmonth_count.rename(columns={month_count.columns[0]: 'Month'}, inplace=True)\n\nmonth_count_fig = ggplot(month_count, aes(x=\"Month\", y=\"OffenseCount\")) + \\\n    geom_line(color='#2e6f40', size=1.5) + \\\n    geom_point(color='#2e6f40', size=3) + \\\n    labs(title='Offense Counts by Month', x='Month', y='Offense Count') + \\\n    scale_x_continuous(breaks=list(range(1, 13))) + \\\n    theme_minimal() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\n# Weekday Count\nweekday_count = pcrime_cleaned.groupby(pcrime_cleaned['OccurDateTime'].dt.weekday)['OffenseCount'].sum().reset_index()\n\nweekday_count.rename(columns={weekday_count.columns[0]: 'Weekday'}, inplace=True)\n\nweekday_count_fig = ggplot(weekday_count, aes(x=\"Weekday\", y=\"OffenseCount\")) + \\\n    geom_line(color='#2e6f40', size=1.5) + \\\n    geom_point(color='#2e6f40', size=3) + \\\n    labs(title='Offense Counts by Weekday', x='Weekday', y='Offense Count') + \\\n    theme_minimal() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\n# Hour Count\nhour_count = pcrime_cleaned.groupby(pcrime_cleaned['OccurDateTime'].dt.hour)['OffenseCount'].sum().reset_index()\n\nhour_count.rename(columns={hour_count.columns[0]: 'Hour'}, inplace=True)\n\nhour_count_fig = ggplot(hour_count, aes(x=\"Hour\", y=\"OffenseCount\")) + \\\n    geom_line(color='#2e6f40', size=1.5) + \\\n    geom_point(color='#2e6f40', size=3) + \\\n    labs(title='Offense Counts by Hour', x='Hour', y='Offense Count') + \\\n    scale_x_continuous(breaks=list(range(0, 25))) + \\\n    theme_minimal() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\n# Create a list of plots to display in a single row\ntemp_plot_list = [\n    month_count_fig,\n    weekday_count_fig,\n    hour_count_fig\n]\n\n# Arrange the plots in a single row\ntemp_plots = gggrid(temp_plot_list, ncol=3) + ggsize(1200, 400)\n\n# Show the combined plot\ntemp_plots\n\n\n   \n   \n\n\n\n\nObservations\nCrime patterns exhibit distinct temporal trends across months, weekdays, and hours. Monthly data shows that crime tends to slow down during the winter months and gradually rises through the summer and into the rest of the year, potentially influenced by seasonal factors such as weather and increased outdoor activity. Looking at weekly patterns, Friday stands out as the day with the highest number of reported offenses, which aligns with the start of the weekend when more people are out, creating more opportunities for crime. Additionally, crime follows a predictable daily cycle, with certain hours experiencing higher offense counts. These trends suggest that external factors like weather, social behavior, and law enforcement presence may play a role in crime fluctuations, warranting further analysis to uncover deeper insights.\n\n\n\nCrime Distribution Trends\n\n\ncrime_against_count\n# Crime Against Count\ncrime_against = pcrime_cleaned.groupby('CrimeAgainst',as_index=False)['OffenseCount'].sum()\ncrime_against_fig = ggplot(crime_against, aes(x='CrimeAgainst', y='OffenseCount')) + \\\n    geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n    labs(title='Offense Counts by Crime Type', x='Crime Against', y='Offense Count') + \\\n    theme_minimal() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\n# Crime Against Report Time\n\ncrime_against_report_box = (ggplot(pcrime_cleaned,aes(x='ReportDiff',y='CrimeAgainst'))+\n  geom_boxplot(outlier_shape = None,fill='#2e6f40', color='black')+\n  scale_x_log10()+\n  theme_minimal()+\n  labs(title = 'Crime Against Report Time Distribution',x= 'Days to Report',y='Crime Against'))\n\ncrime_against_list = [crime_against_fig,crime_against_report_box]\n\ncrime_against_plots = gggrid(crime_against_list,ncol=2)+ ggsize(1600, 600)\n\ncrime_against_plots\n\n\n   \n   \n\n\n\nObservations\nIt‚Äôs evident that crimes against property are much more common than other categories. My initial thought is that property crimes may be more frequent because they‚Äôre often easier to commit, both physically and morally. Property doesn‚Äôt involve direct harm to individuals, which could make it feel less risky or less severe to potential offenders. The differences in report times are also interesting. All crime against types have a median report time of 1 day, however, crime against person has a larger distribution of report times. This also makes sense because many crimes against a person are very sensitive situations that lead to delayed reporting.\n\n\ncrime_category_count\n# Crime Category Count\ncrime_cat = pcrime_cleaned.groupby('OffenseCategory',as_index=False)['OffenseCount'].sum().sort_values(by='OffenseCount', ascending=False).head(10)\ncrime_cat_fig = ggplot(crime_cat, aes(x='OffenseCount', y='OffenseCategory')) + \\\n    geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n    labs(title='Offense Counts by Category', x='Offense COunt', y='Offense Category') + \\\n    theme_minimal() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\ncrime_cat_fig\n\n# Crime Type Count\ncrime_type = pcrime_cleaned.groupby('OffenseType',as_index=False)['OffenseCount'].sum().sort_values(by='OffenseCount', ascending=False).head(10)\ncrime_type_fig = ggplot(crime_type, aes(x='OffenseCount', y='OffenseType')) + \\\n    geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n    labs(title='Offense Counts by Type', x='Offense COunt', y='Offense Type') + \\\n    theme_minimal() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\n\ncrime_type_fig\n\n# Neighborhood Count\nneigh_count = pcrime_cleaned.groupby('Neighborhood',as_index=False)['OffenseCount'].sum().sort_values(by='OffenseCount', ascending=False).head(10)\nneigh_count_fig = ggplot(neigh_count,aes(y=neigh_count[\"Neighborhood\"],x=neigh_count[\"OffenseCount\"]))+ \\\n    geom_bar(stat='identity',fill='#2e6f40', color='black')+ \\\n    labs(title='Offense Count by Neighborhood', x='Offense Count', y='Neighborhood') + \\\n    theme_minimal() + \\\n    theme(\n        plot_title=element_text(size=16, face='bold'),\n        axis_title_x=element_text(size=12, face='bold'),\n        axis_title_y=element_text(size=12, face='bold'),\n        axis_text_x=element_text(size=10), \n        axis_text_y=element_text(size=10),\n        panel_grid_minor=element_blank()\n    )\nneigh_count_fig\n\ncount_list = [crime_cat_fig,crime_type_fig,neigh_count_fig]\n\ncount_plots = gggrid(count_list, ncol=2) + ggsize(1200, 400)\n\ncount_plots\n\n\n   \n   \n\n\n\n\nObservations\nLarceny stands out as the most common offense in Portland, reinforcing the broader trend that property crimes are significantly more prevalent than other crime categories. This may be attributed to the opportunistic nature of larceny‚Äîthese offenses often require little planning and can happen quickly, unlike more serious crimes that demand time, effort, or emotional involvement. Additionally, neighborhood-level analysis shows that Downtown and Hazelwood experience disproportionately high numbers of reported offenses. While this highlights potential crime hotspots, further context‚Äîsuch as population density, neighborhood size, and visitor traffic‚Äîwould provide a more accurate understanding of these patterns.\n\n\ncrime_category_report\n# Crime Category Report Time\n# crime_cat_report = pcrime_cleaned.groupby('OffenseCategory',as_index=False)['ReportDiff'].mean().sort_values(by='ReportDiff', ascending=False).head(10)\n# crime_cat_report_fig = ggplot(pcrime_cleaned, aes(x='ReportDiff', y='OffenseCategory')) + \\\n#     geom_boxplot(coef=0,fill='#2e6f40', color='black') + \\\n#     labs(title='Average Report Time by Category', x='Average Time to Report (Days)', y='Offense Category') + \\\n#     theme_minimal() + \\\n#     coord_cartesian(xlim=(0, 100))+\\\n#     theme(\n#         plot_title=element_text(size=16, face='bold'),\n#         axis_title_x=element_text(size=12, face='bold'),\n#         axis_title_y=element_text(size=12, face='bold'),\n#         axis_text_x=element_text(size=10), \n#         axis_text_y=element_text(size=10),\n#         panel_grid_minor=element_blank()\n#     )\n\n# crime_cat_report_fig\n\n# Crime Category Report Time\n# crime_cat_report = pcrime_cleaned.groupby('OffenseCategory',as_index=False)['ReportDiff'].median().sort_values(by='ReportDiff', ascending=False).head(10)\n# crime_cat_report_fig = ggplot(crime_cat_report, aes(x='ReportDiff', y='OffenseCategory')) + \\\n#     geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n#     labs(title='Average Report Time by Category', x='Average Time to Report (Days)', y='Offense Category') + \\\n#     theme_minimal() + \\\n#     theme(\n#         plot_title=element_text(size=16, face='bold'),\n#         axis_title_x=element_text(size=12, face='bold'),\n#         axis_title_y=element_text(size=12, face='bold'),\n#         axis_text_x=element_text(size=10), \n#         axis_text_y=element_text(size=10),\n#         panel_grid_minor=element_blank()\n#     )\n\n# crime_cat_report_fig\n\n\n\n\n\ncrime_type_report\n# Crime Type Report Time\n# crime_type_report = pcrime_cleaned.groupby('OffenseType',as_index=False)['ReportDiff'].mean().sort_values(by='ReportDiff', ascending=False).head(10)\n# crime_type_report_fig = ggplot(crime_type_report, aes(x='ReportDiff', y='OffenseType')) + \\\n#     geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n#     labs(title='Average Report Time by Type', x='Average Time to Report (Days)', y='Offense Type') + \\\n#     theme_minimal() + \\\n#     theme(\n#         plot_title=element_text(size=16, face='bold'),\n#         axis_title_x=element_text(size=12, face='bold'),\n#         axis_title_y=element_text(size=12, face='bold'),\n#         axis_text_x=element_text(size=10), \n#         axis_text_y=element_text(size=10),\n#         panel_grid_minor=element_blank()\n#     )\n\n# crime_type_report_fig\n\n\n\n\n\ncrime_type_count\n# Crime Type Count\n# crime_type = pcrime_cleaned.groupby('OffenseType',as_index=False)['OffenseCount'].sum().sort_values(by='OffenseCount', ascending=False).head(10)\n# crime_type_fig = ggplot(crime_type, aes(x='OffenseCount', y='OffenseType')) + \\\n#     geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n#     labs(title='Offense Counts by Type', x='Offense COunt', y='Offense Type') + \\\n#     theme_minimal() + \\\n#     theme(\n#         plot_title=element_text(size=16, face='bold'),\n#         axis_title_x=element_text(size=12, face='bold'),\n#         axis_title_y=element_text(size=12, face='bold'),\n#         axis_text_x=element_text(size=10), \n#         axis_text_y=element_text(size=10),\n#         panel_grid_minor=element_blank()\n#     )\n\n# crime_type_fig\n\n\n\n\n\n\nNeighborhood Trends\n\n\nneigh_count\n# Neighborhood Count\n# neigh_count = pcrime_cleaned.groupby('Neighborhood',as_index=False)['OffenseCount'].sum().sort_values(by='OffenseCount', ascending=False).head(10)\n# neigh_count_fig = ggplot(neigh_count,aes(y=neigh_count[\"Neighborhood\"],x=neigh_count[\"OffenseCount\"]))+ \\\n#     geom_bar(stat='identity',fill='#2e6f40', color='black')+ \\\n#     labs(title='Offense Count by Neighborhood', x='Offense Count', y='Neighborhood') + \\\n#     theme_minimal() + \\\n#     theme(\n#         plot_title=element_text(size=16, face='bold'),\n#         axis_title_x=element_text(size=12, face='bold'),\n#         axis_title_y=element_text(size=12, face='bold'),\n#         axis_text_x=element_text(size=10), \n#         axis_text_y=element_text(size=10),\n#         panel_grid_minor=element_blank()\n#     )\n# neigh_count_fig\n\n\n\n\n\nneigh_report\n# Neighborhood Report Time\n# neigh_report = pcrime_cleaned.groupby('Neighborhood',as_index=False)['ReportDiff'].mean().sort_values(by='ReportDiff', ascending=False).head(10)\n# neigh_report_fig = ggplot(neigh_report, aes(x='ReportDiff', y='Neighborhood')) + \\\n#     geom_bar(stat='identity', fill='#2e6f40', color='black') + \\\n#     labs(title='Average Report Time by Neighborhood', x='Average Report Time (Days)', y='Neighborhood') + \\\n#     theme_minimal() + \\\n#     theme(\n#         plot_title=element_text(size=16, face='bold'),\n#         axis_title_x=element_text(size=12, face='bold'),\n#         axis_title_y=element_text(size=12, face='bold'),\n#         axis_text_x=element_text(size=10), \n#         axis_text_y=element_text(size=10),\n#         panel_grid_minor=element_blank()\n#     )\n\n# neigh_report_fig"
  },
  {
    "objectID": "Cleansing_Projects/pcrime copy.html#current-final-observations",
    "href": "Cleansing_Projects/pcrime copy.html#current-final-observations",
    "title": "Portland Crime Analysis",
    "section": "Current Final Observations",
    "text": "Current Final Observations\nThis project is still a work in progress, and there‚Äôs plenty more to explore and visualize to better address some of the questions raised during my analysis. That said, the work completed so far has revealed some fascinating insights into crime patterns in Portland, Oregon. Thank you for taking the time to check out my project‚ÄîI hope you‚Äôll return to see the updates as it continues to develop!"
  },
  {
    "objectID": "resume copy.html",
    "href": "resume copy.html",
    "title": "Tucker Trost‚Äôs CV",
    "section": "",
    "text": "Phone: (971) 420-5397\nEmail: Tuckertrost16@gmail.com\nLinkedIn: linkedin.com/in/tuckertrost\n\n\n\nBrigham Young University‚ÄìIdaho, Rexburg, ID\nBachelor of Science in Data Science, Minor in Statistics ‚Äî Expected Fall 2025\nRelevant Courses: Machine Learning, Data Visualization, Data Wrangling, Statistics, SQL, Python, R, Pyspark\nGoogle Advanced Data Analytics Professional Certificate ‚Äî Winter 2023\nSkills: Python, SQL, Machine Learning, Statistics\nGoogle Data Analytics Professional Certificate ‚Äî Summer 2023\nSkills: Microsoft Excel, SQL, Tableau, R, Data Cleaning, Data Integrity\n\n\n\n\nAnalysis & Visualization: Python (Pandas, lets-plot, Plotly, Pyspark), R (ggplot2, tidyverse), Power BI (DAX), Tableau\nMachine Learning: Scikit-learn, TensorFlow, Random Forest, CNNs, XGBoost\nDatabase Management & Design: MySQL, SQLite, Databricks, DAX Studio, Microsoft SQL Server\n\n\n\n\nData Visualization Specialist ‚Äî Brigham Young University‚ÄìIdaho\nStudent Records and Registration | Oct 2024 ‚Äì Present\n- Took ownership of a high-impact records retention dashboard project, stepping in midstream with no formal handoff and quickly becoming the subject matter expert. - Collaborated closely with department leadership, presenting biweekly progress updates and implementing their feedback to align the final solution with organizational needs and AACRAO standards.\nProject Manager ‚Äî Data Science Society\n- Led a team of student data scientists to scope, gather, and analyze data for an ongoing project, ensuring alignment with stakeholder objectives. - Facilitated regular stakeholder meetings to refine problem statements, define deliverables, and provide progress updates, driving actionable insights.\nData Science Tutor ‚Äî Data Science Tutoring Lab\n- Guided students of all skill levels in Python and R, breaking down data wrangling and visualization concepts to enhance comprehension beyond assignments. - Adapted explanations to diverse learning styles, fostering problem-solving skills and confidence in data analysis.\nTeacher‚Äôs Assistant ‚Äî Data Insight and Intuition\n- Graded assignments and projects, providing constructive feedback to help students improve. - Guided students in developing clear, compelling data stories through personalized critiques and support.\n\n\n\n\n\n\n\nDesigned and built a Power BI model that identified over 200,000 student records ready for disposal, streamlining compliance with AACRAO retention guidelines.\nCreated a structured database schema to support efficient querying and documentation of the retention model‚Äôs logic.\nWrangled data from multiple sources using Python before integrating it into the Power BI model, ensuring accuracy and consistency in record retention tracking.\n\n\n\n\n\nBuilt an XGBoost regression model with Scikit-Learn to predict daily offense counts by Portland neighborhood using temporal and spatial features.\nApplied feature engineering, hyperparameter tuning, and model evaluation with Python to improve prediction accuracy and reveal crime patterns."
  },
  {
    "objectID": "resume copy.html#education",
    "href": "resume copy.html#education",
    "title": "Tucker Trost‚Äôs CV",
    "section": "",
    "text": "Brigham Young University‚ÄìIdaho, Rexburg, ID\nBachelor of Science in Data Science, Minor in Statistics ‚Äî Expected Fall 2025\nRelevant Courses: Machine Learning, Data Visualization, Data Wrangling, Statistics, SQL, Python, R, Pyspark\nGoogle Advanced Data Analytics Professional Certificate ‚Äî Winter 2023\nSkills: Python, SQL, Machine Learning, Statistics\nGoogle Data Analytics Professional Certificate ‚Äî Summer 2023\nSkills: Microsoft Excel, SQL, Tableau, R, Data Cleaning, Data Integrity"
  },
  {
    "objectID": "resume copy.html#skills",
    "href": "resume copy.html#skills",
    "title": "Tucker Trost‚Äôs CV",
    "section": "",
    "text": "Analysis & Visualization: Python (Pandas, lets-plot, Plotly, Pyspark), R (ggplot2, tidyverse), Power BI (DAX), Tableau\nMachine Learning: Scikit-learn, TensorFlow, Random Forest, CNNs, XGBoost\nDatabase Management & Design: MySQL, SQLite, Databricks, DAX Studio, Microsoft SQL Server"
  },
  {
    "objectID": "resume copy.html#experience",
    "href": "resume copy.html#experience",
    "title": "Tucker Trost‚Äôs CV",
    "section": "",
    "text": "Data Visualization Specialist ‚Äî Brigham Young University‚ÄìIdaho\nStudent Records and Registration | Oct 2024 ‚Äì Present\n- Took ownership of a high-impact records retention dashboard project, stepping in midstream with no formal handoff and quickly becoming the subject matter expert. - Collaborated closely with department leadership, presenting biweekly progress updates and implementing their feedback to align the final solution with organizational needs and AACRAO standards.\nProject Manager ‚Äî Data Science Society\n- Led a team of student data scientists to scope, gather, and analyze data for an ongoing project, ensuring alignment with stakeholder objectives. - Facilitated regular stakeholder meetings to refine problem statements, define deliverables, and provide progress updates, driving actionable insights.\nData Science Tutor ‚Äî Data Science Tutoring Lab\n- Guided students of all skill levels in Python and R, breaking down data wrangling and visualization concepts to enhance comprehension beyond assignments. - Adapted explanations to diverse learning styles, fostering problem-solving skills and confidence in data analysis.\nTeacher‚Äôs Assistant ‚Äî Data Insight and Intuition\n- Graded assignments and projects, providing constructive feedback to help students improve. - Guided students in developing clear, compelling data stories through personalized critiques and support."
  },
  {
    "objectID": "resume copy.html#projects",
    "href": "resume copy.html#projects",
    "title": "Tucker Trost‚Äôs CV",
    "section": "",
    "text": "Designed and built a Power BI model that identified over 200,000 student records ready for disposal, streamlining compliance with AACRAO retention guidelines.\nCreated a structured database schema to support efficient querying and documentation of the retention model‚Äôs logic.\nWrangled data from multiple sources using Python before integrating it into the Power BI model, ensuring accuracy and consistency in record retention tracking.\n\n\n\n\n\nBuilt an XGBoost regression model with Scikit-Learn to predict daily offense counts by Portland neighborhood using temporal and spatial features.\nApplied feature engineering, hyperparameter tuning, and model evaluation with Python to improve prediction accuracy and reveal crime patterns."
  },
  {
    "objectID": "resume.html#skills",
    "href": "resume.html#skills",
    "title": "Tucker Trost",
    "section": "",
    "text": "Analysis/Visualization: Python (Pandas, Lets-Plot, plotly, Pyspark), R (ggplot, tidyverse), Power BI (DAX), Tableau\nMachine Learning: Scikit-learn, TensorFlow, Random Forest Classifiers, Neural Networks, XGBoost\nDatabase Management/Design: MySQL, SQLite, Databricks, DAX Studio, Microsoft SQL Server"
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Tucker Trost",
    "section": "",
    "text": "Data Analyst, Student Records and Registration\nBrigham Young University-Idaho ‚Äî Idaho\nOctober 2024 ‚Äì Present\n- Built and maintained a records retention Power BI Dashboard identifying over 400,000 student records ready for disposal\n- Utilized SQL to pull data from multiple sources to pipeline into dashboard for automation\n- Presented dashboard updates to executives biweekly for feedback and stakeholder alignment\nData Science Consultant, BYU-I Career Center\nBrigham Young University-Idaho ‚Äî Idaho\nApril 2025 ‚Äì Present\n- Built an end-to-end data pipeline and Power BI dashboard to track alumni career outcomes\n- Wrote SQL and Python code to pull, clean, and process data from multiple sources, including NLP on job titles\n- Met regularly with stakeholders to align on goals, share updates, and deliver actionable insights\nProject Manager, Data Science Society\nBrigham Young University-Idaho ‚Äî Idaho\nJanuary 2025 ‚Äì April 2025\n- Led a team of data scientists using a SQL Database to house data for an attendance analytics project for the Pioneer Baseball League\n- Created compelling Data Visualizations in Python for stakeholders\n- Met once a month with league commissioner to communicate insights and receive feedback\nData Science Tutor/Teaching Assistant\nBrigham Young University-Idaho ‚Äî Idaho\nJanuary 2025 ‚Äì April 2025\n- Guided students of all skill levels in Python, SQL, R, Tableau, and Power BI\n- Broke down data wrangling and visualization concepts to enhance comprehension beyond assignments\n- Adapted explanations to diverse learning styles, fostering problem-solving skills and confidence in data analysis"
  },
  {
    "objectID": "resume.html#data-scientist",
    "href": "resume.html#data-scientist",
    "title": "Tucker Trost",
    "section": "",
    "text": "Phone: (971) 420-5397\nEmail: Tuckertrost16@gmail.com\nLinkedIn: linkedin.com/in/tuckertrost"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Data Science Blog",
    "section": "",
    "text": "Starting My Journey in NLP\n\n\n\nDate: May 5th, 2025\nLately I‚Äôve been diving into Natural Language Processing (NLP) for my senior project. I‚Äôve been teaching myself the basics through podcasts, books, and working with Python‚Äôs NLTK library.\nIt‚Äôs been super cool to see how widely NLP is used. Everything from sentiment analysis to search engines to chatbots. There is so much information to be extracted from text and so much data in the world is text.\nTo help with my learning, I made a quick NLP vocab cheat sheet with some core terms. Figured I‚Äôd share in case it could help anyone else.\nExcited to continue learning and sharing my progress as I go!\n\n\n\nNLP CheatSheet",
    "crumbs": [
      "Blog"
    ]
  },
  {
    "objectID": "blog.html#title-2-header",
    "href": "blog.html#title-2-header",
    "title": "Blog",
    "section": "",
    "text": "My Blog Post Title\n\n\n\nDate: 2025-05-14\nTags: Data Science, Resume, Markdown\nHere is a short summary of the post.\n\n\nMarkDown Basics",
    "crumbs": [
      "Blog"
    ]
  },
  {
    "objectID": "blog.html#natural-language-processing",
    "href": "blog.html#natural-language-processing",
    "title": "My Data Science Blog",
    "section": "",
    "text": "Starting My Journey in NLP\n\n\n\nDate: May 5th, 2025\nLately I‚Äôve been diving into Natural Language Processing (NLP) for my senior project. I‚Äôve been teaching myself the basics through podcasts, books, and working with Python‚Äôs NLTK library.\nIt‚Äôs been super cool to see how widely NLP is used. Everything from sentiment analysis to search engines to chatbots. There is so much information to be extracted from text and so much data in the world is text.\nTo help with my learning, I made a quick NLP vocab cheat sheet with some core terms. Figured I‚Äôd share in case it could help anyone else.\nExcited to continue learning and sharing my progress as I go!\n\n\n\nNLP CheatSheet",
    "crumbs": [
      "Blog"
    ]
  }
]